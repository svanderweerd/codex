{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 This is my digital garden . This is where I write. Keep track of my thoughts, be it lingering or active ones. Write down ideas, concepts and topics that interest me or that I might need in the future. In the past I've tried to take notes in a notebook, on my ipad, but also using third party applications such as Obsidian and Notion . Alas, as time passes, I find myself incapable of actively maintaining either one of them, or extract meaningful and longterm value from them. The personal notebook or Ipad notes become cluttered and unorganized, losing my motivation to use it as a personal library. Similar feelings arose when using Notion.so. Building comprehensive tables, code snippets, and referring to other pages often became a mess as well. I found that Obisidan grew on me - I even purschased a yearly subscription to support the developers. It has a steep learning curve, but eventually I got the hang of it. Unfortunately, this become somewhat difficult for me to maintain as well: it is self hosted on my laptop, which I don't all the time. Instead, I often work from devices that are provided by my clients, making it difficult to actively work in Obsidian. Additionally, I can't seem to use Obsidian for light-hearted topics such as fitness, or use it to put in my fleeting notes. I can't shake the feeling that I need to use Obsidian for academic grade documentation only, or use it to document advanced topics. This ultimately led me back to old school note-taking, scattering my thoughts all over the place (Notion, Obsidian, physical notebooks, and Apple Notes). With digital garden, I embark on my next journey of collecting and maintaining my thoughts; a backup of my brains, so to speak. As Gwern put it: The goal of these pages is not to be a model of concision, maximizing entertainment value per word, or to preach to a choir by elegantly repeating a conclusion. Rather, I am attempting to explain things to my future self, who is intelligent and interested, but has forgotten. What I am doing is explaining why I decided what I did to myself and noting down everything I found interesting about it for future reference. I hope my other readers, whomever they may be, might find the topic as interesting as I found it, and the essay useful or at least entertaining\u2013but the intended audience is my future self. Principles \u00b6 Based on Gwern's long site principle . Heavily inspired by Lyz Digital Garden .","title":"Introduction"},{"location":"#introduction","text":"This is my digital garden . This is where I write. Keep track of my thoughts, be it lingering or active ones. Write down ideas, concepts and topics that interest me or that I might need in the future. In the past I've tried to take notes in a notebook, on my ipad, but also using third party applications such as Obsidian and Notion . Alas, as time passes, I find myself incapable of actively maintaining either one of them, or extract meaningful and longterm value from them. The personal notebook or Ipad notes become cluttered and unorganized, losing my motivation to use it as a personal library. Similar feelings arose when using Notion.so. Building comprehensive tables, code snippets, and referring to other pages often became a mess as well. I found that Obisidan grew on me - I even purschased a yearly subscription to support the developers. It has a steep learning curve, but eventually I got the hang of it. Unfortunately, this become somewhat difficult for me to maintain as well: it is self hosted on my laptop, which I don't all the time. Instead, I often work from devices that are provided by my clients, making it difficult to actively work in Obsidian. Additionally, I can't seem to use Obsidian for light-hearted topics such as fitness, or use it to put in my fleeting notes. I can't shake the feeling that I need to use Obsidian for academic grade documentation only, or use it to document advanced topics. This ultimately led me back to old school note-taking, scattering my thoughts all over the place (Notion, Obsidian, physical notebooks, and Apple Notes). With digital garden, I embark on my next journey of collecting and maintaining my thoughts; a backup of my brains, so to speak. As Gwern put it: The goal of these pages is not to be a model of concision, maximizing entertainment value per word, or to preach to a choir by elegantly repeating a conclusion. Rather, I am attempting to explain things to my future self, who is intelligent and interested, but has forgotten. What I am doing is explaining why I decided what I did to myself and noting down everything I found interesting about it for future reference. I hope my other readers, whomever they may be, might find the topic as interesting as I found it, and the essay useful or at least entertaining\u2013but the intended audience is my future self.","title":"Introduction"},{"location":"#principles","text":"Based on Gwern's long site principle . Heavily inspired by Lyz Digital Garden .","title":"Principles"},{"location":"coding/architecture/ddd/","text":"Domain Driven Design \u00b6 Nice reddit post discussing domain driven design. Another interesting post is provided by microsoft where they talk about ddd and Command and Query Responsibility Segregation (CQRS).","title":"Domain Driven Design"},{"location":"coding/architecture/ddd/#domain-driven-design","text":"Nice reddit post discussing domain driven design. Another interesting post is provided by microsoft where they talk about ddd and Command and Query Responsibility Segregation (CQRS).","title":"Domain Driven Design"},{"location":"coding/architecture/entity-relationship/","text":"This post will cover my understanding on entity relationship modelling. There are several types of entity relations: one-to-many: a Customer can have several Orders whereas an Order can't have several Customers. many-to-many: an Author can have several Books, but a Book can have several Authors as well. One to Many \u00b6 One-to-many (or many-to-one) relationships are the most common type of database relationships. A timeless example of how such a relationship is applied is a business' relationship between customers & orders . Single customers have multiple orders, but orders don't have multiple customers, hence the term. Now, building on our Blog site, we can understand that authors might have multiple posts, and that posts might have multiple comments. These are two typical one-to-many relationships. Many to Many \u00b6 However, we might also have a data model where an Author might have multiple Post and a Post might have multiple Authors. Foreign Keys \u00b6","title":"Entity relationship modelling"},{"location":"coding/architecture/entity-relationship/#one-to-many","text":"One-to-many (or many-to-one) relationships are the most common type of database relationships. A timeless example of how such a relationship is applied is a business' relationship between customers & orders . Single customers have multiple orders, but orders don't have multiple customers, hence the term. Now, building on our Blog site, we can understand that authors might have multiple posts, and that posts might have multiple comments. These are two typical one-to-many relationships.","title":"One to Many"},{"location":"coding/architecture/entity-relationship/#many-to-many","text":"However, we might also have a data model where an Author might have multiple Post and a Post might have multiple Authors.","title":"Many to Many"},{"location":"coding/architecture/entity-relationship/#foreign-keys","text":"","title":"Foreign Keys"},{"location":"coding/architecture/repository-pattern/","text":"This section draws heavily from Martin Fowler's Patterns of Enterprise Architecture Applications, and Domain Driven Design . A system with a complex domain model (the place where the business logic is put) often benefits from a layer, such as the one provided by Data Mapper , that isolates domain objects from details of the database access code. In such systems it can be worthwhile to build another layer of abstraction over the mapping layer where query construction code is concentrated. This becomes more important when there are a large number of domain classes or heavy querying. In these cases particularly, adding this layer helps minimize duplicate query logic. A Repository mediates between the domain and data mapping layers, acting like an in-memory domain object collection. Client objects construct query specifications declaratively and submit them to Repository for satisfaction. Objects can be added to and removed from the Repository, as they can from a simple collection of objects, and the mapping code encapsulated by the Repository will carry out the appropriate operations behind the scenes. Conceptually, a Repository encapsulates the set of objects persisted in a data store and the operations performed over them, providing a more object-oriented view of the persistence layer. Repository also supports the objective of achieving a clean separation and one-way dependency between the domain and data mapping layers. Info A repository can be considered an interface that serves as your apps's gateway to the database, in terms of object-relational mappings.","title":"Repository pattern"},{"location":"coding/basics/","text":"This is my first line. Learning methods \u00b6 Section about the different ways to learn coding and your experiences with it.","title":"Coding basics"},{"location":"coding/basics/#learning-methods","text":"Section about the different ways to learn coding and your experiences with it.","title":"Learning methods"},{"location":"coding/basics/setup-coding-env/","text":"This page covers the setup of your coding environment. Setting up your coding environment \u00b6 Include the following in this section: Note For language specific setup of your development environment, refer to the language specific sections in your Documentation. For example, if you want to start with Python, go to the Python setup page in the Python section. Tools to consider: Editor (e.g., VSC or Intellij IDEA) Version control (e.g., Git, Github, Gitlab) Learning the basics (e.g., searching for information) Editor \u00b6 Version Control \u00b6 Learning the Basics \u00b6","title":"Setup coding environment"},{"location":"coding/basics/setup-coding-env/#setting-up-your-coding-environment","text":"Include the following in this section: Note For language specific setup of your development environment, refer to the language specific sections in your Documentation. For example, if you want to start with Python, go to the Python setup page in the Python section. Tools to consider: Editor (e.g., VSC or Intellij IDEA) Version control (e.g., Git, Github, Gitlab) Learning the basics (e.g., searching for information)","title":"Setting up your coding environment"},{"location":"coding/basics/setup-coding-env/#editor","text":"","title":"Editor"},{"location":"coding/basics/setup-coding-env/#version-control","text":"","title":"Version Control"},{"location":"coding/basics/setup-coding-env/#learning-the-basics","text":"","title":"Learning the Basics"},{"location":"coding/python/","text":"Layout \u00b6 I am actively using Python for several use case: transform and load complex Excel files. web scrawling (e.g., Funda scraper). web application for IT Risk Management (based on FastAPI). My Python docs will focus on concepts that I have encountered throughout the development of my use cases, in contrast to covering topics that I have learned throughout several courses (e.g., bootcamp courses on Udemy). The reasoning behind this, is that focus on topics that I encountered will resonate better with me; after all, I have used or needed them when building stuff. Developer roadmap \u00b6 There is a good Python developer roadmap on Reddit if you are new to Python (it can be found on Github as well). The roadmap covers the following concepts: Data structures Data management Data flows OOP Language skeleton Multithreading & Multiprocessing Common practices Algorithms Databases Web Architecture References \u00b6 Hackers & Slackers provides alot of material related to Python. Examples include: working with Excel, SQLAlchemy, GraphQL, Web Scraping, SQL native, and more.","title":"Python"},{"location":"coding/python/#layout","text":"I am actively using Python for several use case: transform and load complex Excel files. web scrawling (e.g., Funda scraper). web application for IT Risk Management (based on FastAPI). My Python docs will focus on concepts that I have encountered throughout the development of my use cases, in contrast to covering topics that I have learned throughout several courses (e.g., bootcamp courses on Udemy). The reasoning behind this, is that focus on topics that I encountered will resonate better with me; after all, I have used or needed them when building stuff.","title":"Layout"},{"location":"coding/python/#developer-roadmap","text":"There is a good Python developer roadmap on Reddit if you are new to Python (it can be found on Github as well). The roadmap covers the following concepts: Data structures Data management Data flows OOP Language skeleton Multithreading & Multiprocessing Common practices Algorithms Databases Web Architecture","title":"Developer roadmap"},{"location":"coding/python/#references","text":"Hackers & Slackers provides alot of material related to Python. Examples include: working with Excel, SQLAlchemy, GraphQL, Web Scraping, SQL native, and more.","title":"References"},{"location":"coding/python/data-structures/","text":"Everything in Python is an object. Each object has its own data attributes and methods associated with it. In order to use an object efficiently and appropriately, we should know how to interact with them. Lists, tuples, and sets are 3 important types of objects. What they have in common is that they are used as data structures. In order to create robust and well-performing products, one must know the data structures of a programming language very well. Data structures that I use the most are: list tuple dict set array & bytes range dataclass Lists, tuples & sets \u00b6 list is a built-in data structure in Python. It is represented as a collection of data points in square brackets. Lists can be used to store any data type or a mixture of different data types. Lists are mutable which is one of the reasons why they are so commonly used. tuple is a collection of values separated by comma and enclosed in parentheses. Unlike lists, tuples are immutable. The immutability can be considered as the identifying feature of tuples. set is an unordered collection of distinct immutable objects. A set contains unique elements. Although sets are mutable, the elements of sets must be immutable. There is no order associated with the elements of a set. Thus, it does not support indexing or slicing like we do with lists. my_list = [ 1 , 2 , \"3\" ] my_set = { 1 , 2 , \"3\" } my_tuple = ( 1 , 2 , \"3\" ) List vs Sets \u00b6 Consider the following: text = \"Hello World!\" print ( list ( text )) [ 'H' , 'e' , 'l' , 'l' , 'o' , ' ' , 'W' , 'o' , 'r' , 'l' , 'd' , '!' ] print ( set ( text )) { 'H' , 'W' , 'o' , ' ' , 'l' , 'r' , '!' , 'e' , 'd' } As you can see above, the first print statement returns a list (i) with all the characters in the text variable and (ii) ordered based on the order of the chars in the text string. The second statement returns a set (i) containing only unique characters, as a set doesn't allow duplicate values. Secondly, (ii) a set is unordered, hence the random positions of the chars in the set. In the previous example, we saw that sets do not possess an order. Thus, we cannot do slicing or indexing on sets like we do with lists. The example below demonstrates this: text = \"Hello World!\" list_a = list ( text ) print ( list_a [: 2 ]) # print item in list from index position 0 up until (but excluding) 2 [ 'H' , 'e' ] set_a = set ( text ) print ( set_a [: 2 ]) # returns a TypeError: 'set' object is not subscriptable List vs Tuples \u00b6 The difference between list and tuple is the mutability. Unlike lists, tuples are immutable. For instance, we can add items to a list but cannot do it with tuples. The methods that change a collection (e.g. append, remove, extend, pop) are not applicable to tuples, as shown below: list_a : list [ int ] = [ 1 , 2 , 3 , 4 ] # apply type specification list_a . append ( 5 ) print ( list_a ) [ 1 , 2 , 3 , 4 , 5 ] tuple_a = ( 1 , 2 , 3 , 4 ) tuple_a . append ( 5 ) # AttributeError: 'tuple' object has no attribute 'append' The immutability might be the most identifying feature of tuples. We cannot (re)assign a value to an item of a tuple: tuple_a = ( 3 , 5 , 'x' , 5 ) tuple_a [ 0 ] = 7 # will generate an error Although tuples are immutable, they can contain mutable elements such as lists or sets: tuple_a = ([ 1 , 3 ], 'a' , 'b' , 8 ) # this tuple contains a list with 2 items tuple_a [ 0 ][ 0 ] = 99 # we reassign our first item in the list from value 1 to 99 print ( tuple_a ) ([ 99 , 3 ], 'a' , 'b' , 8 ) Working with lists, sets, and tuples \u00b6 References \u00b6 15 examples to master Python lists, sets and tuples Difference between del, remove, and pop *","title":"Data structures"},{"location":"coding/python/data-structures/#lists-tuples-sets","text":"list is a built-in data structure in Python. It is represented as a collection of data points in square brackets. Lists can be used to store any data type or a mixture of different data types. Lists are mutable which is one of the reasons why they are so commonly used. tuple is a collection of values separated by comma and enclosed in parentheses. Unlike lists, tuples are immutable. The immutability can be considered as the identifying feature of tuples. set is an unordered collection of distinct immutable objects. A set contains unique elements. Although sets are mutable, the elements of sets must be immutable. There is no order associated with the elements of a set. Thus, it does not support indexing or slicing like we do with lists. my_list = [ 1 , 2 , \"3\" ] my_set = { 1 , 2 , \"3\" } my_tuple = ( 1 , 2 , \"3\" )","title":"Lists, tuples &amp; sets"},{"location":"coding/python/data-structures/#list-vs-sets","text":"Consider the following: text = \"Hello World!\" print ( list ( text )) [ 'H' , 'e' , 'l' , 'l' , 'o' , ' ' , 'W' , 'o' , 'r' , 'l' , 'd' , '!' ] print ( set ( text )) { 'H' , 'W' , 'o' , ' ' , 'l' , 'r' , '!' , 'e' , 'd' } As you can see above, the first print statement returns a list (i) with all the characters in the text variable and (ii) ordered based on the order of the chars in the text string. The second statement returns a set (i) containing only unique characters, as a set doesn't allow duplicate values. Secondly, (ii) a set is unordered, hence the random positions of the chars in the set. In the previous example, we saw that sets do not possess an order. Thus, we cannot do slicing or indexing on sets like we do with lists. The example below demonstrates this: text = \"Hello World!\" list_a = list ( text ) print ( list_a [: 2 ]) # print item in list from index position 0 up until (but excluding) 2 [ 'H' , 'e' ] set_a = set ( text ) print ( set_a [: 2 ]) # returns a TypeError: 'set' object is not subscriptable","title":"List vs Sets"},{"location":"coding/python/data-structures/#list-vs-tuples","text":"The difference between list and tuple is the mutability. Unlike lists, tuples are immutable. For instance, we can add items to a list but cannot do it with tuples. The methods that change a collection (e.g. append, remove, extend, pop) are not applicable to tuples, as shown below: list_a : list [ int ] = [ 1 , 2 , 3 , 4 ] # apply type specification list_a . append ( 5 ) print ( list_a ) [ 1 , 2 , 3 , 4 , 5 ] tuple_a = ( 1 , 2 , 3 , 4 ) tuple_a . append ( 5 ) # AttributeError: 'tuple' object has no attribute 'append' The immutability might be the most identifying feature of tuples. We cannot (re)assign a value to an item of a tuple: tuple_a = ( 3 , 5 , 'x' , 5 ) tuple_a [ 0 ] = 7 # will generate an error Although tuples are immutable, they can contain mutable elements such as lists or sets: tuple_a = ([ 1 , 3 ], 'a' , 'b' , 8 ) # this tuple contains a list with 2 items tuple_a [ 0 ][ 0 ] = 99 # we reassign our first item in the list from value 1 to 99 print ( tuple_a ) ([ 99 , 3 ], 'a' , 'b' , 8 )","title":"List vs Tuples"},{"location":"coding/python/data-structures/#working-with-lists-sets-and-tuples","text":"","title":"Working with lists, sets, and tuples"},{"location":"coding/python/data-structures/#references","text":"15 examples to master Python lists, sets and tuples Difference between del, remove, and pop *","title":"References"},{"location":"coding/python/python-setup/","text":"This page describes the setup of Python on a Macbook. Managing Python environments \u00b6 Although Python is pre-installed on MacBooks, you don't want to use the system's version or any single Python version to manage your Python apps. You will install a lot of packages, and ideally you want them segregated based on use case i.e. project. Hence, we will make use of Pyenv , which we can install using Homebrew: brew install pyenv Adding pyenv to your shell environment \u00b6 After install pyenv, you need to add it to your shell environment so that your shell ( zsh ) is able to find the package. To setup the shell environment, run the following in your shell: echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.zshrc echo 'command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.zshrc echo 'eval \"$(pyenv init -)\"' >> ~/.zshrc Install Python versions \u00b6 To install other Python versions, simply use pyenv install <Python version> . In the example below we install Python 3.10.6: pyenv install 3 .10.6 pyenv install -l returns a list of all Python version you can install. Python version that you install, are located in the /Users/<user>/.pyenv/versions directory. Switch between Python versions \u00b6 Use pyenv versions to see which Python versions you have installed. Simply switch to an installed version by using the commands: pyenv shell <version> -- select for the current shell that you have active. pyenv local <version> -- automatically select whenever you are in the current directory. pyenv global <version> -- set a global (i.e., default) Python version for your MacBook user. Dependency & Package management \u00b6 There are a couple of things that we have tried in the past when we wanted to setup a package & environment manager for Python. For instance, we have tried conda, miniconda, mamba and poetry. Based on our experience with these services, I have decided to stick with Poetry . A Redditor has the following to say about it: Note For larger projects package building and containerisation are preferred. Most people suggest the use of poetry and pyproject.toml (not setup.py, which is outdated) to build a package that is then uploaded to a private package server. This package is then installed into a docker container, which can be deployed. In general, Python applications should be packaged, as the use of packaging allows for richer metadata and more robust handling of dependencies as well as the ability to install the package directly at a later date if need be. No one has suggested the use of conda for deployment, which I was surprised by given its prevalence in the data science community. Additionally, check out this article on why you should use poetry over other package and dependency managers. Poetry & Pyenv \u00b6 If you are having trouble ensuring Poetry uses the correct Python version, check out this Github issue, which goes into more detail.","title":"Python setup"},{"location":"coding/python/python-setup/#managing-python-environments","text":"Although Python is pre-installed on MacBooks, you don't want to use the system's version or any single Python version to manage your Python apps. You will install a lot of packages, and ideally you want them segregated based on use case i.e. project. Hence, we will make use of Pyenv , which we can install using Homebrew: brew install pyenv","title":"Managing Python environments"},{"location":"coding/python/python-setup/#adding-pyenv-to-your-shell-environment","text":"After install pyenv, you need to add it to your shell environment so that your shell ( zsh ) is able to find the package. To setup the shell environment, run the following in your shell: echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.zshrc echo 'command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.zshrc echo 'eval \"$(pyenv init -)\"' >> ~/.zshrc","title":"Adding pyenv to your shell environment"},{"location":"coding/python/python-setup/#install-python-versions","text":"To install other Python versions, simply use pyenv install <Python version> . In the example below we install Python 3.10.6: pyenv install 3 .10.6 pyenv install -l returns a list of all Python version you can install. Python version that you install, are located in the /Users/<user>/.pyenv/versions directory.","title":"Install Python versions"},{"location":"coding/python/python-setup/#switch-between-python-versions","text":"Use pyenv versions to see which Python versions you have installed. Simply switch to an installed version by using the commands: pyenv shell <version> -- select for the current shell that you have active. pyenv local <version> -- automatically select whenever you are in the current directory. pyenv global <version> -- set a global (i.e., default) Python version for your MacBook user.","title":"Switch between Python versions"},{"location":"coding/python/python-setup/#dependency-package-management","text":"There are a couple of things that we have tried in the past when we wanted to setup a package & environment manager for Python. For instance, we have tried conda, miniconda, mamba and poetry. Based on our experience with these services, I have decided to stick with Poetry . A Redditor has the following to say about it: Note For larger projects package building and containerisation are preferred. Most people suggest the use of poetry and pyproject.toml (not setup.py, which is outdated) to build a package that is then uploaded to a private package server. This package is then installed into a docker container, which can be deployed. In general, Python applications should be packaged, as the use of packaging allows for richer metadata and more robust handling of dependencies as well as the ability to install the package directly at a later date if need be. No one has suggested the use of conda for deployment, which I was surprised by given its prevalence in the data science community. Additionally, check out this article on why you should use poetry over other package and dependency managers.","title":"Dependency &amp; Package management"},{"location":"coding/python/python-setup/#poetry-pyenv","text":"If you are having trouble ensuring Poetry uses the correct Python version, check out this Github issue, which goes into more detail.","title":"Poetry &amp; Pyenv"},{"location":"coding/python/libraries/alembic/","text":"A db migration tool.","title":"Alembic"},{"location":"coding/python/libraries/pydantic/","text":"Pydantic is a validation & modelling thing.","title":"Pydantic"},{"location":"coding/python/libraries/sqlalchemy/","text":"SQLAlchemy is an ORM built with Domain Driven Design in mind post . It can be used to perform ad hoc querying on your database (db), but also used as an ORM for larger applications. SA has 2 components: Core and ORM . Core concepts \u00b6 Declarative mapping \u00b6 We want to define tables and columns from our Python classes using the ORM. In SQLAlchemy, this is enabled through a declarative mapping . The most common pattern is constructing a base class using the SQLALchemy declarative_base function, and then having all DB model classes inherit from this base class. To enable us to create tables and columns from our data classes, we need to construct a mapping between a class (i.e., Model) and the table (and it's columns). Below features a declarative base which is then used in a declarative table mapping: from sqlalchemy import Column , Integer , String , ForeignKey from sqlalchemy.orm import declarative_base # declarative base class Base = declarative_base () Above, we instantiate a base class with the return object provided by the declarative_base callable. We can then use this Base class when constructing our Models (i.e., tables): # an example mapping using the base class class User ( Base ): __tablename__ = \"user\" id = Column ( Integer , primary_key = True ) name = Column ( String ) fullname = Column ( String ) nickname = Column ( String ) You can use a decorator as well (also provided by SA). This enables you to declare helper methods on the Base class, like automatically creating a __tablename__ . Using a decorator will look like the following: from sqlalchemy.orm import as_declarative @as_declarative () class Base ( object ): @declared_attr def __tablename__ ( cls ): return cls . __name__ . lower () id = Column ( Integer , primary_key = True ) The decorator form of mapping is particularly useful when combining a SQLAlchemy declarative mapping with other forms of class declaration, notably the Python dataclasses module or Pydantic. The engine \u00b6 The start of any SQLAlchemy application is an object called the Engine . This object acts as a central source of connections to a particular database, providing both a factory and a holding space called a connection pool for these database connections. The engine is typically a global object created just once for a particular database server. The Engine is created by using create_engine() (see the SA docs as well); we also set the flag future to True , since we want to make use of SA's new API version: from sqlalchemy import create_engine engine = create_engine ( \"sqlite+pysqlite:///:memory:\" , echo = True , future = True ) The main argument to create_engine() is our DB connection URI, which tells the engine what kind of db we have and where the db is located. Session \u00b6 Interacting directly with the engine might be okay for simple tasks, but when you are building a bigger application it might be better to start with the ORM from the get-go. By using the ORM, you basically abstract away the SQL stuff. You interact with the db using Model objects, i.e. data classes. In this case, Models represent your db's tables, the attributes of Models represent the table's columns. A Session is a persistent database connection that makes it easier to communicate with our database. The session begins in a mostly stateless form; once the session object is called, it requests a connection resource from the engine that it is bound to. Sessions are created by binding them to an SQLAlchemy engine, which we covered in the previous part. With an engine created, all we need is to use SQLAlchemy's sessionmaker to define a session and bind it to our engine: \"\"\"database.py\"\"\" \"\"\"Database engine & session creation.\"\"\" from sqlalchemy import create_engine # noqa from sqlalchemy.orm import sessionmaker # noqa engine = create_engine ( 'mysql+pymysql://user:password@host:3600/database' , echo = True ) Session = sessionmaker ( bind = engine ) session = Session () On sessionmaker Session is a regular Python class which can be directly instantiated. However, to standardize how sessions are configured and acquired,the sessionmaker class is normally used to create a top level Session configuration which can then be used throughout an application without the need to repeat the configuration arguments. Models (i.e. tables) \u00b6 The User class example that is shown below, is a Model that is used to represent a users table in our database. You can see that we have imported several SQLAlchemy types , which we see getting passed into each Column . Each type corresponds to a SQL data type. Thus, our SQL table columns' data types would be integer , varchar(255) , and text , respectively. Columns can also accept optional parameters to things like keys or column constraints: primary_key: Designates a column as the table's \"primary key,\" a highly recommended practice that serves as a unique identifier as well as an index for SQL to search on. autoincrement: Only relevant to columns that are both the primary_key and have the type Integer . Each user we create will automatically be assigned an id , where our first user will have an id of 1, and subsequent users would increment accordingly. unique: Places a constraint where no two records/rows share the same value for the given column (we don't want two users to have the same username). nullable: When set to True , adds a constraint that the column is mandatory, and no row will be created unless a value is provided. key: Places a secondary key on the given column, typically used in tandem with another constraint such as index . index: Designates that a column's values are sortable in a non-arbitrary way in the interest of improving query performance. server_default: A default value to assign if a value is not explicitly passed. Models & Schemas When building Python api/crud apps, we often use both SQLAlchemy & Pydantic . Both packages use the concept of Models when referring to their data classes. To differentiate between them, I'll refer to my Pydanctic Models as schemas and my sqlalchemy models as models . When you create an instance of a model (e.g., you create a user), you are effectively creating a new row in the associated db table. But before you can do that, you first need to make sure that you have a table already. Now, you can either create that by (i) accessing your database and create a table manually, (ii) use SA's built-in method on your Base class that creates the tables for you (see code sample below); or (iii) make use of migration tool such as Alembic . # continues from above Base . metadata . create_all ( engine ) Storing data using Models & Sessions \u00b6 With a model defined and session created, we have the luxury of adding and modifying data purely in Python. SQLAlchemy refers to this as function-based query construction . To create a record in our users db table, we need to create an instance of our User model, and use our session to pass our instance to the engine: from models import User # noqa from database import session # noqa user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) session . add ( user ) # add the user session . commit () # commit the change With an instance of User created, all it takes to create this user in our database are two calls to our session: add() queues the item for creation, and commit() saves the change. We should now see a row in our database's user table! Working with session is as easy as four simple methods: session.add() : We can pass an instance of a data model into add() to quickly create a new record to be added to our database. session.delete() : Like the above, delete() accepts an instance of a data model. If that record exists in our database, it will be staged for deletion. session.commit() : Changes made within a session are not saved until explicitly committed. session.close() : Unlike SQLAlchemy engines, sessions are connections that remain open until explicitly closed. Entity relationships in SQLAlchemy \u00b6 SA provides several features that allow you to define entity relations in your app's data model. To understand relationships in SA , imagine that we build an app for blogging where we need three models: from sqlalchemy import declarative_base , Column , Integer , Text , String , DateTime , Boolean # noqa from sqlalchemy.sql import func # noqa class User (): \"\"\"User account.\"\"\" __tablename__ = \"user\" id = Column ( Integer , primary_key = True , autoincrement = \"auto\" ) username = Column ( String ( 255 ), unique = True , nullable = False ) password = Column ( Text , nullable = False ) email = Column ( String ( 255 ), unique = True , nullable = False ) class Comment (): \"\"\"User-generated comment on a blog post.\"\"\" __tablename__ = \"comment\" id = Column ( Integer , primary_key = True , index = True ) user_id = Column ( Integer ) post_id = Column ( Integer , index = True ) body = Column ( Text ) upvotes = Column ( Integer , default = 1 ) removed = Column ( Boolean , default = False ) created_at = Column ( DateTime , server_default = func . now ()) class Post (): \"\"\"Blog post.\"\"\" __tablename__ = \"post\" id = Column ( Integer , primary_key = True , index = True ) author_id = Column ( Integer ) slug = Column ( String ( 255 ), nullable = False , unique = True ) title = Column ( String ( 255 ), nullable = False ) summary = Column ( String ( 400 )) feature_image = Column ( String ( 300 )) body = Column ( Text ) status = Column ( String ( 255 ), nullable = False , default = \"unpublished\" ) created_at = Column ( DateTime , server_default = func . now ()) updated_at = Column ( DateTime , server_default = func . now ()) One-to-many \u00b6 We use the relationship function to provide a relationship between two mapped classes. Consider it some kind of \"magic\" attribute that will contain the values from other tables related to this one. We update our Post and Comment Models to implement the relationship accordingly: ... class Comment (): \"\"\"User-generated comment on a blog post.\"\"\" __tablename__ = \"comment\" id = Column ( Integer , primary_key = True , index = True ) user_id = Column ( Integer , ForeignKey ( \"user.id\" )) # ---- (1.i) ---- post_id = Column ( Integer , ForeignKey ( \"post.id\" ), index = True ) # ---- (1.ii) ---- body = Column ( Text ) upvotes = Column ( Integer , default = 1 ) removed = Column ( Boolean , default = False ) created_at = Column ( DateTime , server_default = func . now ()) # Relationships user = relationship ( \"User\" ) # ---- (2.i) ---- class Post (): \"\"\"Blog post.\"\"\" __tablename__ = \"post\" id = Column ( Integer , primary_key = True , index = True ) author_id = Column ( Integer , ForeignKey ( \"user.id\" )) # ---- (1.iii) ---- slug = Column ( String ( 255 ), nullable = False , unique = True ) title = Column ( String ( 255 ), nullable = False ) summary = Column ( String ( 400 )) body = Column ( Text ) status = Column ( String ( 255 ), nullable = False , default = \"unpublished\" ) created_at = Column ( DateTime , server_default = func . now ()) updated_at = Column ( DateTime , server_default = func . now ()) # Relationships author = relationship ( \"User\" ) # ---- (2.ii) ---- comments = relationship ( \"Comment\" ) # ---- (2.iii) ---- As shown above, we have added several one-to-many relationships using the (1) ForeignKey and (2) relationship() concepts: We've set some attributes (i.e., columns) as Foreign Keys with ForeignKey property.We basically tie data between our tables so that fetching one will allow us to get information about the other. Our comment table has a foreign key user.id linked to its user_id attribute, indicating that the commenter has a record in the user table. This table also has a foreign key post.id linked to its post_id attribute, indicating that a comment should have a relation to an existing post in the post table. Lastly, we note that our post table has a foreign key user.id linked to its author_id attribute, indicating a relation with the user table. The other new concept here is relationships. Relationships complement foreign keys and are a way of telling our application (not our database) that we're building relationships between two Models (instead of db tables!). In our Comment Model we define a relationship with the User model and assign it to user . Similarly, in our Post Model we define a relationship between our author attribute and the User Model, and We define a relationship between our comments atttribute and the Comment Model. On FKs and relationships() Foreign keys tell our database which relationships we're building, and relationships tell our app which relationships we're building. The point of all this is the ability to easily perform JOINs in our app. When using an ORM, we wouldn't be able to say \"join this Model with that Model \", because our app would have no idea which columns (Model attributes) to join on. When our relationships are specified in our Models, we can do things like join two tables together without specifying any further detail: SQLAlchemy will know how to join tables/models by looking at what we set in our data models (through foreign keys & relationships). Tip SQLAlchemy only creates tables from data models if the tables don't already exist. In other words, if we have faulty relationships the first time we run our app, the error messages will persist the second time we run our app, even if we think we've fixed the problem. To deal with strange error messages, try deleting your SQL tables before running your app again whenever making changes to a model. Back references \u00b6 Specifying relationships on a data model allows us to access properties of the linked model via a property on the original model. If we were to join our Comment model with our User model, we'd be able to access the properties of a comment's author via Comment.user.username , where user is the name of our relationship, and username is a property of the associated model (the User model in this case). Relationships created in this way are one-directional, in that the Comment model can access the User props through its user prop, but the User model can't access the props of the Comment model. SA provides the backref param to enable just that, making it a bidirectional relationship. Let's update our Post Model: author = relationship ( \"User\" , backref = \"posts\" ) We can now access user details of a post by calling Post.author . Go to the section Code example - working with our models & SA \u00b6 We are fist going to instantiate 1 User and 2 Post objects: # objects.py admin_user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) post_1 = Post ( author_id = admin_user . id , slug = \"fake-post-slug\" , title = \"fake title\" , summary = \"a post example\" , body = \"lorem ipsum blabla. This contains the actual text of the post\" , status = \"published\" ) post_2 = Post ( author_id = admin_user . id , slug = \"an-additional-post\" , title = \"Yet Another Post Title\" , summary = \"An in-depth exploration into writing your second blog post.\" , body = \"Smelly cheese cheese slices fromage\" , status = \"published\" , ) Now we want to create functions that allow us to save our objects to the database: # orm.py def create_user ( session : Session , user : User ) -> User : \"\"\" Create a new user if username isn't already taken. :param session: SQLAlchemy database session. :type session: Session :param user: New user record to create. :type user: User :return: Optional[User] \"\"\" try : existing_user = session . query ( User ) . filter ( User . username == user . username ) . first () if existing_user is None : session . add ( user ) # Add the user session . commit () # Commit the change LOGGER . success ( f \"Created user: { user } \" ) else : LOGGER . warning ( f \"Users already exists in database: { existing_user } \" ) return session . query ( User ) . filter ( User . username == user . username ) . first () except IntegrityError as e : LOGGER . error ( e . orig ) raise e . orig except SQLAlchemyError as e : LOGGER . error ( f \"Unexpected error when creating user: { e } \" ) raise e def create_post ( session : Session , post : Post ) -> Post : \"\"\" Create a post. :param session: SQLAlchemy database session. :type session: Session :param post: Blog post to be created. :type post: Post :return: Post \"\"\" try : existing_post = session . query ( Post ) . filter ( Post . slug == post . slug ) . first () if existing_post is None : session . add ( post ) # Add the post session . commit () # Commit the change LOGGER . success ( f \"Created post { post } published by user { post . author . username } \" ) return session . query ( Post ) . filter ( Post . slug == post . slug ) . first () else : LOGGER . warning ( f \"Post already exists in database: { post } \" ) return existing_post except IntegrityError as e : LOGGER . error ( e . orig ) raise e . orig except SQLAlchemyError as e : LOGGER . error ( f \"Unexpected error when creating user: { e } \" ) raise e First we pass our objects to the crud functions to store them in our db, and subsequently we define a function that allows us to retrieve the data from our db. # objects.py ... admin_user = create_user ( session , admin_user ) post_1 = create_post ( session , post_1 ) post_2 = create_post ( session , post_2 ) # orm.py ... def get_all_posts ( session : Session , admin_user : User ): \"\"\" Fetch all posts belonging to an author user. :param session: SQLAlchemy database session. :type session: Session :param admin_user: Author of blog posts. :type admin_user: User :return: None \"\"\" posts = ( session . query ( Post ) # (1) . join ( User , Post . author_id == User . id ) # (2) . filter_by ( username = admin_user . username ) # (3) . all () ) In our get_all_posts function, we: query our db table associated with the Post model (i.e., the post table). We then JOIN posts with user using the .join(User, Post.author_id == User.id) param. Here, we basically 'paste' the columns of the user table \"behind\" the columns of the post table. To ensure that the values are associated with the correct record (i..e, author), you tie both data fields together by assigning the Post.author_id to User.id . Lastly, we fetch all posts in our database belonging to our user admin_user : we tell the db to only get the data where the username attribute in our user table equal the admin_user.username value (Bob-12 in our example). References \u00b6 General: Hackers & Slackers Data relationships in SQLAlchemy:","title":"SQLAlchemy"},{"location":"coding/python/libraries/sqlalchemy/#core-concepts","text":"","title":"Core concepts"},{"location":"coding/python/libraries/sqlalchemy/#declarative-mapping","text":"We want to define tables and columns from our Python classes using the ORM. In SQLAlchemy, this is enabled through a declarative mapping . The most common pattern is constructing a base class using the SQLALchemy declarative_base function, and then having all DB model classes inherit from this base class. To enable us to create tables and columns from our data classes, we need to construct a mapping between a class (i.e., Model) and the table (and it's columns). Below features a declarative base which is then used in a declarative table mapping: from sqlalchemy import Column , Integer , String , ForeignKey from sqlalchemy.orm import declarative_base # declarative base class Base = declarative_base () Above, we instantiate a base class with the return object provided by the declarative_base callable. We can then use this Base class when constructing our Models (i.e., tables): # an example mapping using the base class class User ( Base ): __tablename__ = \"user\" id = Column ( Integer , primary_key = True ) name = Column ( String ) fullname = Column ( String ) nickname = Column ( String ) You can use a decorator as well (also provided by SA). This enables you to declare helper methods on the Base class, like automatically creating a __tablename__ . Using a decorator will look like the following: from sqlalchemy.orm import as_declarative @as_declarative () class Base ( object ): @declared_attr def __tablename__ ( cls ): return cls . __name__ . lower () id = Column ( Integer , primary_key = True ) The decorator form of mapping is particularly useful when combining a SQLAlchemy declarative mapping with other forms of class declaration, notably the Python dataclasses module or Pydantic.","title":"Declarative mapping"},{"location":"coding/python/libraries/sqlalchemy/#the-engine","text":"The start of any SQLAlchemy application is an object called the Engine . This object acts as a central source of connections to a particular database, providing both a factory and a holding space called a connection pool for these database connections. The engine is typically a global object created just once for a particular database server. The Engine is created by using create_engine() (see the SA docs as well); we also set the flag future to True , since we want to make use of SA's new API version: from sqlalchemy import create_engine engine = create_engine ( \"sqlite+pysqlite:///:memory:\" , echo = True , future = True ) The main argument to create_engine() is our DB connection URI, which tells the engine what kind of db we have and where the db is located.","title":"The engine"},{"location":"coding/python/libraries/sqlalchemy/#session","text":"Interacting directly with the engine might be okay for simple tasks, but when you are building a bigger application it might be better to start with the ORM from the get-go. By using the ORM, you basically abstract away the SQL stuff. You interact with the db using Model objects, i.e. data classes. In this case, Models represent your db's tables, the attributes of Models represent the table's columns. A Session is a persistent database connection that makes it easier to communicate with our database. The session begins in a mostly stateless form; once the session object is called, it requests a connection resource from the engine that it is bound to. Sessions are created by binding them to an SQLAlchemy engine, which we covered in the previous part. With an engine created, all we need is to use SQLAlchemy's sessionmaker to define a session and bind it to our engine: \"\"\"database.py\"\"\" \"\"\"Database engine & session creation.\"\"\" from sqlalchemy import create_engine # noqa from sqlalchemy.orm import sessionmaker # noqa engine = create_engine ( 'mysql+pymysql://user:password@host:3600/database' , echo = True ) Session = sessionmaker ( bind = engine ) session = Session () On sessionmaker Session is a regular Python class which can be directly instantiated. However, to standardize how sessions are configured and acquired,the sessionmaker class is normally used to create a top level Session configuration which can then be used throughout an application without the need to repeat the configuration arguments.","title":"Session"},{"location":"coding/python/libraries/sqlalchemy/#models-ie-tables","text":"The User class example that is shown below, is a Model that is used to represent a users table in our database. You can see that we have imported several SQLAlchemy types , which we see getting passed into each Column . Each type corresponds to a SQL data type. Thus, our SQL table columns' data types would be integer , varchar(255) , and text , respectively. Columns can also accept optional parameters to things like keys or column constraints: primary_key: Designates a column as the table's \"primary key,\" a highly recommended practice that serves as a unique identifier as well as an index for SQL to search on. autoincrement: Only relevant to columns that are both the primary_key and have the type Integer . Each user we create will automatically be assigned an id , where our first user will have an id of 1, and subsequent users would increment accordingly. unique: Places a constraint where no two records/rows share the same value for the given column (we don't want two users to have the same username). nullable: When set to True , adds a constraint that the column is mandatory, and no row will be created unless a value is provided. key: Places a secondary key on the given column, typically used in tandem with another constraint such as index . index: Designates that a column's values are sortable in a non-arbitrary way in the interest of improving query performance. server_default: A default value to assign if a value is not explicitly passed. Models & Schemas When building Python api/crud apps, we often use both SQLAlchemy & Pydantic . Both packages use the concept of Models when referring to their data classes. To differentiate between them, I'll refer to my Pydanctic Models as schemas and my sqlalchemy models as models . When you create an instance of a model (e.g., you create a user), you are effectively creating a new row in the associated db table. But before you can do that, you first need to make sure that you have a table already. Now, you can either create that by (i) accessing your database and create a table manually, (ii) use SA's built-in method on your Base class that creates the tables for you (see code sample below); or (iii) make use of migration tool such as Alembic . # continues from above Base . metadata . create_all ( engine )","title":"Models (i.e. tables)"},{"location":"coding/python/libraries/sqlalchemy/#storing-data-using-models-sessions","text":"With a model defined and session created, we have the luxury of adding and modifying data purely in Python. SQLAlchemy refers to this as function-based query construction . To create a record in our users db table, we need to create an instance of our User model, and use our session to pass our instance to the engine: from models import User # noqa from database import session # noqa user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) session . add ( user ) # add the user session . commit () # commit the change With an instance of User created, all it takes to create this user in our database are two calls to our session: add() queues the item for creation, and commit() saves the change. We should now see a row in our database's user table! Working with session is as easy as four simple methods: session.add() : We can pass an instance of a data model into add() to quickly create a new record to be added to our database. session.delete() : Like the above, delete() accepts an instance of a data model. If that record exists in our database, it will be staged for deletion. session.commit() : Changes made within a session are not saved until explicitly committed. session.close() : Unlike SQLAlchemy engines, sessions are connections that remain open until explicitly closed.","title":"Storing data using Models &amp; Sessions"},{"location":"coding/python/libraries/sqlalchemy/#entity-relationships-in-sqlalchemy","text":"SA provides several features that allow you to define entity relations in your app's data model. To understand relationships in SA , imagine that we build an app for blogging where we need three models: from sqlalchemy import declarative_base , Column , Integer , Text , String , DateTime , Boolean # noqa from sqlalchemy.sql import func # noqa class User (): \"\"\"User account.\"\"\" __tablename__ = \"user\" id = Column ( Integer , primary_key = True , autoincrement = \"auto\" ) username = Column ( String ( 255 ), unique = True , nullable = False ) password = Column ( Text , nullable = False ) email = Column ( String ( 255 ), unique = True , nullable = False ) class Comment (): \"\"\"User-generated comment on a blog post.\"\"\" __tablename__ = \"comment\" id = Column ( Integer , primary_key = True , index = True ) user_id = Column ( Integer ) post_id = Column ( Integer , index = True ) body = Column ( Text ) upvotes = Column ( Integer , default = 1 ) removed = Column ( Boolean , default = False ) created_at = Column ( DateTime , server_default = func . now ()) class Post (): \"\"\"Blog post.\"\"\" __tablename__ = \"post\" id = Column ( Integer , primary_key = True , index = True ) author_id = Column ( Integer ) slug = Column ( String ( 255 ), nullable = False , unique = True ) title = Column ( String ( 255 ), nullable = False ) summary = Column ( String ( 400 )) feature_image = Column ( String ( 300 )) body = Column ( Text ) status = Column ( String ( 255 ), nullable = False , default = \"unpublished\" ) created_at = Column ( DateTime , server_default = func . now ()) updated_at = Column ( DateTime , server_default = func . now ())","title":"Entity relationships in SQLAlchemy"},{"location":"coding/python/libraries/sqlalchemy/#one-to-many","text":"We use the relationship function to provide a relationship between two mapped classes. Consider it some kind of \"magic\" attribute that will contain the values from other tables related to this one. We update our Post and Comment Models to implement the relationship accordingly: ... class Comment (): \"\"\"User-generated comment on a blog post.\"\"\" __tablename__ = \"comment\" id = Column ( Integer , primary_key = True , index = True ) user_id = Column ( Integer , ForeignKey ( \"user.id\" )) # ---- (1.i) ---- post_id = Column ( Integer , ForeignKey ( \"post.id\" ), index = True ) # ---- (1.ii) ---- body = Column ( Text ) upvotes = Column ( Integer , default = 1 ) removed = Column ( Boolean , default = False ) created_at = Column ( DateTime , server_default = func . now ()) # Relationships user = relationship ( \"User\" ) # ---- (2.i) ---- class Post (): \"\"\"Blog post.\"\"\" __tablename__ = \"post\" id = Column ( Integer , primary_key = True , index = True ) author_id = Column ( Integer , ForeignKey ( \"user.id\" )) # ---- (1.iii) ---- slug = Column ( String ( 255 ), nullable = False , unique = True ) title = Column ( String ( 255 ), nullable = False ) summary = Column ( String ( 400 )) body = Column ( Text ) status = Column ( String ( 255 ), nullable = False , default = \"unpublished\" ) created_at = Column ( DateTime , server_default = func . now ()) updated_at = Column ( DateTime , server_default = func . now ()) # Relationships author = relationship ( \"User\" ) # ---- (2.ii) ---- comments = relationship ( \"Comment\" ) # ---- (2.iii) ---- As shown above, we have added several one-to-many relationships using the (1) ForeignKey and (2) relationship() concepts: We've set some attributes (i.e., columns) as Foreign Keys with ForeignKey property.We basically tie data between our tables so that fetching one will allow us to get information about the other. Our comment table has a foreign key user.id linked to its user_id attribute, indicating that the commenter has a record in the user table. This table also has a foreign key post.id linked to its post_id attribute, indicating that a comment should have a relation to an existing post in the post table. Lastly, we note that our post table has a foreign key user.id linked to its author_id attribute, indicating a relation with the user table. The other new concept here is relationships. Relationships complement foreign keys and are a way of telling our application (not our database) that we're building relationships between two Models (instead of db tables!). In our Comment Model we define a relationship with the User model and assign it to user . Similarly, in our Post Model we define a relationship between our author attribute and the User Model, and We define a relationship between our comments atttribute and the Comment Model. On FKs and relationships() Foreign keys tell our database which relationships we're building, and relationships tell our app which relationships we're building. The point of all this is the ability to easily perform JOINs in our app. When using an ORM, we wouldn't be able to say \"join this Model with that Model \", because our app would have no idea which columns (Model attributes) to join on. When our relationships are specified in our Models, we can do things like join two tables together without specifying any further detail: SQLAlchemy will know how to join tables/models by looking at what we set in our data models (through foreign keys & relationships). Tip SQLAlchemy only creates tables from data models if the tables don't already exist. In other words, if we have faulty relationships the first time we run our app, the error messages will persist the second time we run our app, even if we think we've fixed the problem. To deal with strange error messages, try deleting your SQL tables before running your app again whenever making changes to a model.","title":"One-to-many"},{"location":"coding/python/libraries/sqlalchemy/#back-references","text":"Specifying relationships on a data model allows us to access properties of the linked model via a property on the original model. If we were to join our Comment model with our User model, we'd be able to access the properties of a comment's author via Comment.user.username , where user is the name of our relationship, and username is a property of the associated model (the User model in this case). Relationships created in this way are one-directional, in that the Comment model can access the User props through its user prop, but the User model can't access the props of the Comment model. SA provides the backref param to enable just that, making it a bidirectional relationship. Let's update our Post Model: author = relationship ( \"User\" , backref = \"posts\" ) We can now access user details of a post by calling Post.author . Go to the section","title":"Back references"},{"location":"coding/python/libraries/sqlalchemy/#code-example-working-with-our-models-sa","text":"We are fist going to instantiate 1 User and 2 Post objects: # objects.py admin_user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) post_1 = Post ( author_id = admin_user . id , slug = \"fake-post-slug\" , title = \"fake title\" , summary = \"a post example\" , body = \"lorem ipsum blabla. This contains the actual text of the post\" , status = \"published\" ) post_2 = Post ( author_id = admin_user . id , slug = \"an-additional-post\" , title = \"Yet Another Post Title\" , summary = \"An in-depth exploration into writing your second blog post.\" , body = \"Smelly cheese cheese slices fromage\" , status = \"published\" , ) Now we want to create functions that allow us to save our objects to the database: # orm.py def create_user ( session : Session , user : User ) -> User : \"\"\" Create a new user if username isn't already taken. :param session: SQLAlchemy database session. :type session: Session :param user: New user record to create. :type user: User :return: Optional[User] \"\"\" try : existing_user = session . query ( User ) . filter ( User . username == user . username ) . first () if existing_user is None : session . add ( user ) # Add the user session . commit () # Commit the change LOGGER . success ( f \"Created user: { user } \" ) else : LOGGER . warning ( f \"Users already exists in database: { existing_user } \" ) return session . query ( User ) . filter ( User . username == user . username ) . first () except IntegrityError as e : LOGGER . error ( e . orig ) raise e . orig except SQLAlchemyError as e : LOGGER . error ( f \"Unexpected error when creating user: { e } \" ) raise e def create_post ( session : Session , post : Post ) -> Post : \"\"\" Create a post. :param session: SQLAlchemy database session. :type session: Session :param post: Blog post to be created. :type post: Post :return: Post \"\"\" try : existing_post = session . query ( Post ) . filter ( Post . slug == post . slug ) . first () if existing_post is None : session . add ( post ) # Add the post session . commit () # Commit the change LOGGER . success ( f \"Created post { post } published by user { post . author . username } \" ) return session . query ( Post ) . filter ( Post . slug == post . slug ) . first () else : LOGGER . warning ( f \"Post already exists in database: { post } \" ) return existing_post except IntegrityError as e : LOGGER . error ( e . orig ) raise e . orig except SQLAlchemyError as e : LOGGER . error ( f \"Unexpected error when creating user: { e } \" ) raise e First we pass our objects to the crud functions to store them in our db, and subsequently we define a function that allows us to retrieve the data from our db. # objects.py ... admin_user = create_user ( session , admin_user ) post_1 = create_post ( session , post_1 ) post_2 = create_post ( session , post_2 ) # orm.py ... def get_all_posts ( session : Session , admin_user : User ): \"\"\" Fetch all posts belonging to an author user. :param session: SQLAlchemy database session. :type session: Session :param admin_user: Author of blog posts. :type admin_user: User :return: None \"\"\" posts = ( session . query ( Post ) # (1) . join ( User , Post . author_id == User . id ) # (2) . filter_by ( username = admin_user . username ) # (3) . all () ) In our get_all_posts function, we: query our db table associated with the Post model (i.e., the post table). We then JOIN posts with user using the .join(User, Post.author_id == User.id) param. Here, we basically 'paste' the columns of the user table \"behind\" the columns of the post table. To ensure that the values are associated with the correct record (i..e, author), you tie both data fields together by assigning the Post.author_id to User.id . Lastly, we fetch all posts in our database belonging to our user admin_user : we tell the db to only get the data where the username attribute in our user table equal the admin_user.username value (Bob-12 in our example).","title":"Code example - working with our models &amp; SA"},{"location":"coding/python/libraries/sqlalchemy/#references","text":"General: Hackers & Slackers Data relationships in SQLAlchemy:","title":"References"},{"location":"devops/","text":"All things ops related. From editor stuff to postgresql database admin.","title":"DevOps"},{"location":"health/fitness/","text":"This is page is meant to document my thoughts on fitness and how to optimise your work outs alongside a full time job and other activities that you want to pursue. Schemas \u00b6 I currently do full body workouts, split between two different schemas. Each schema has two major exercises: the low bar squat and the deadlift . Each workout also has pull-ups with the purpose of breaking through the plateau of 25 reps in total. Tips & Tricks \u00b6 Squats \u00b6 Squats seem straightforward, but are actually one of the most technical exercises that you can do. To maintain a good technique and optimise for gaining weights, I have created a list op tips and tricks below. These tricks assume a low bar position. Don't use a pad to soften the bar on your back! This impairs your technique and causes you to lift less weight (the pad absorbs part of the weight) Do shoulder rotation exercises before you start. The low bar position might make your shoulder uncomfortable. When your shoulder starts getting too uncomfortable you might naturally bend your wrist to compensate. To counteract that, use a thumbless grip (thumbs over the bar). This allows you to maintain a straight wrist position and lock the bar. Don't move your elbows back and forth. This moves the bar and fucks up your technique. To lock your elbows, act like you are bending the barbell towards ths sides of your chest. Move downwards in a smooth way. Don't go slow, use the momentum in your muscles at the bottom, to go up again. Think of 'bouncing' at the bottom. Use that momentum. Going too slow will impair your technique. When you are at the bottom, move your ass up first. Think of a cord that is attached to your ass. This will train the hamstring and glutes better! youtube vids on squat technique Check our the following videos by Alan Thrall on squat technique: Low bar squat - Rack position & Shoulder warm-ups How to squat - Low bar","title":"Fitness"},{"location":"health/fitness/#schemas","text":"I currently do full body workouts, split between two different schemas. Each schema has two major exercises: the low bar squat and the deadlift . Each workout also has pull-ups with the purpose of breaking through the plateau of 25 reps in total.","title":"Schemas"},{"location":"health/fitness/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"health/fitness/#squats","text":"Squats seem straightforward, but are actually one of the most technical exercises that you can do. To maintain a good technique and optimise for gaining weights, I have created a list op tips and tricks below. These tricks assume a low bar position. Don't use a pad to soften the bar on your back! This impairs your technique and causes you to lift less weight (the pad absorbs part of the weight) Do shoulder rotation exercises before you start. The low bar position might make your shoulder uncomfortable. When your shoulder starts getting too uncomfortable you might naturally bend your wrist to compensate. To counteract that, use a thumbless grip (thumbs over the bar). This allows you to maintain a straight wrist position and lock the bar. Don't move your elbows back and forth. This moves the bar and fucks up your technique. To lock your elbows, act like you are bending the barbell towards ths sides of your chest. Move downwards in a smooth way. Don't go slow, use the momentum in your muscles at the bottom, to go up again. Think of 'bouncing' at the bottom. Use that momentum. Going too slow will impair your technique. When you are at the bottom, move your ass up first. Think of a cord that is attached to your ass. This will train the hamstring and glutes better! youtube vids on squat technique Check our the following videos by Alan Thrall on squat technique: Low bar squat - Rack position & Shoulder warm-ups How to squat - Low bar","title":"Squats"},{"location":"health/sleep/","text":"Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli. References This section is mostly extracted from Lyz's article on Sleep , which in turn is based on the why we sleep book by Matthew Walker .","title":"Sleep"},{"location":"projects/glab-analyzer/","text":"A pyhon project that focuses on the extraction and analyses of the Gitlab events. Setup \u00b6 The setup of the project is as follows: Consumes Gitlab built-in API Fetches - on glab project basis - the MR's, Push events Determine unreviewed MRs and direct push to release branches Outputs Excel file with the results 3 rd party libs used \u00b6 The following libraries are used: glab == Gitlab's python library pandas == For analyzing data and building Excels jira == for searching and pushing excels to Jira issues Documentation \u00b6 Gitlab API docs","title":"Gitlab analyzer"},{"location":"projects/glab-analyzer/#setup","text":"The setup of the project is as follows: Consumes Gitlab built-in API Fetches - on glab project basis - the MR's, Push events Determine unreviewed MRs and direct push to release branches Outputs Excel file with the results","title":"Setup"},{"location":"projects/glab-analyzer/#3rd-party-libs-used","text":"The following libraries are used: glab == Gitlab's python library pandas == For analyzing data and building Excels jira == for searching and pushing excels to Jira issues","title":"3rd party libs used"},{"location":"projects/glab-analyzer/#documentation","text":"Gitlab API docs","title":"Documentation"},{"location":"tooling/bash/","text":"# fetch which processes listen or have a conn established on a port lsof -i | grep -E \"(LISTEN|ESTABLISHED)\" # check which process specifically listens/established a conn on port 8000. # You can use any port number as input lsof -i :8000 # kill process based on process ID kill -9 <PID>","title":"Bash"},{"location":"tooling/intellij-idea/","text":"Although IntelliJ IDEA is tailored for Java and JVM, it is a multi-language IDE that supports Rust, Javascript, Python, SQL, and numerous other languages. Unicode strings for apple symbols: Sym Key \u2303 Control \u2325 Option \u21e7 Shift \u2318 Command Navigation \u00b6 Action Description \u2303 + [ Move caret backwards a paragraph \u2303 + ] Move caret forwards a paragraph \u2318 + l Jump to line:column \u2318 + e Go to recent files \u2318 + \u21e7 + a Opens up actions \u2325 + enter Opens up suggestions pane (yellow lightbulb) options for the code where you caret is \u2318 + 1 Go to Project sidebar \u2325 + f12 Go to terminal","title":"Intellij IDEA"},{"location":"tooling/intellij-idea/#navigation","text":"Action Description \u2303 + [ Move caret backwards a paragraph \u2303 + ] Move caret forwards a paragraph \u2318 + l Jump to line:column \u2318 + e Go to recent files \u2318 + \u21e7 + a Opens up actions \u2325 + enter Opens up suggestions pane (yellow lightbulb) options for the code where you caret is \u2318 + 1 Go to Project sidebar \u2325 + f12 Go to terminal","title":"Navigation"},{"location":"tooling/setup-mkdocs/","text":"Getting started with Mkdocs \u00b6 This page explains how to use MkDocs and host it on Github Pages. Create mkdocs site \u00b6 pip3 install mkdocs installs the mkdocs package to generate python based static sites. mkdocs new [site-name] creates a new project with the correct set-up. mkdocs serve - Start the live-reloading docs server. mkdocs build subsequently generates your static pages and places the files in /sites . Make sure to include this in your .gitignore file. Setting up a Python environment \u00b6 Here we explain how to set up a virtual environment using pyenv and use requirements.txt to keep track of our dependencies. Instead of using pyenv you can also directly start with Poetry. Further down this guide we explain the steps regarding a Poetry set up. setup a python environment ( pyenv virtualenv [environmentName] ) and activate it with pyenv activate [environmentName] . Install packages and plugins accordingly. Run pip freeze > requirements.txt to output your deps into requirements.txt . Add .venv to your .gitignore file. Host on gh pages \u00b6 Create a new repository in Github (empty one). Perform git init in your local mkdocs folder (the root folder that stores the mkdocs.yml file). The usual stuff (git add, git commit, git remote add origin [remote-url] and git push -u origin main). Now you need to perform the mkdocs gh-deploy command in your local mkdocs project. This creates a Git branch names gh-pages for Github Pages to pick up your site. Running this command will generate the static website. Rather than outputting the files in the site folder as we saw in the previous post, the website will be saved in a new branch named gh-pages and a push of this branch is done towards GitHub. If you go to the Actions tab on the GitHub repository, you will see GitHub automatically picking up the new gh-pages branch and deploying it to GitHub pages. Now your site is hosted on github pages accordingly. Everything has been set up! Automated deployments \u00b6 So far I have described how to manually deploy your mkdocs site to Github Pages. The next steps explain how you can do this automatically through Github Actions. Go to your Github repo and then to Actions menu. Click on New workflow and select the suggested workflow (simple workflow). This will generate a new file called blank.yml in the .github/workflows directory. See the example below for the changes that we conducted on the template file. I suggest to copy this over. Please be mindful of your default branch's name! If you have master , make sure to reflect that in the yaml file below as well. --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt - name : Build site run : mkdocs gh-deploy --force --clean --verbose Change the file name to gh-pages.yml and make sure to commit this change to the default branch. Remember that the gh-pages branch will be generated by MkDocs, and we never want to either check out this branch locally or modify it manually. Now you have a minimal set-up for deploying your site through Github Actions automatically. The next steps refines our Github Workflow approach. These steps are not really needed if you don't want to. You can continue using pip3 with pyenv (or venv) for dependency and environment management, instead of poetry. You may also decide to add a Deploy job without integrating with Poetry all together. You simply change the yml file as follows: # I will not include the stuff that won't be changed - name : Build site run : mkdocs build --verbose # here I have updated the code from gh-deploy to build # When you replace gh-deploy, you need to add the step below in order to push the site to gh-pages: - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} # ----- SEE NEXT SECTION FOR DEPLOYMENT KEYS! ----- publish_dir : ./site Refining Github Workflow \u00b6 After our first deployment, we do a git fetch and git pull in our local project. This will update our local folder and adds the newly created contents inside the .github directory. Before we use Poetry locally, we first need to make sure that we even have a Poetry environment and pyproject. toml file in our root directory. We run poetry init , and install the dependencies that are listed in our requirements.txt file. Now we tweak the gh-pages.yml file as follows: --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install Poetry uses : snok/install-poetry@v1 with : virtualenvs-create : true virtualenvs-in-project : true installer-parallel : true - name : Load cached venv id : cached-poetry-dependencies uses : actions/cache@v3 with : path : .venv key : venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }} - name : Install dependencies if cache doesnt exist if : steps.cached-poetry-dependencies.outputs.cache-hit != 'true' run : poetry install --no-interaction --no-root - name : Install project run : poetry install --no-interaction --no-root - name : Make the site run : poetry run mkdocs build --verbose # run: poetry run mkdocs gh-deploy --force --clean --verbose - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./site As shown above, we add a couple of jobs that take care of installing Poetry and our project dependencies. We also replace our run command in the Make the site job: we add poetry but also replace gh-deploy with build . Again, you may choose to keep poetry run mkdocs gh-deploy . Doing so, renders the job Deploy redundant. I suggest to remove that job accordingly. Since we are newly introducing Poetry to our pipeline, it might be good to first test if this is integrated correctly, and we don't experience any issues. If everything is fine, we can work on the next step. Deploy job \u00b6 Now that we don't use the gh-deploy functionality provided by MkDocs , we need to add another job that takes care of pushing our site to the gh-pages branch. That's where the Deploy job comes in. You'll notice the following parameter: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} . Because we don't make use of the gh-deploy functionality, we need to make use of Github Runners : agents that are used to deploy code to certain environments. In our case, it is the gh-pages environment (i.e., branch). However, we need to properly authenticate the Runner otherwise it will not be able to perform write operations, thus deploying our site. To ensure that the Runner is able to authenticate, we make use of deploy keys and add them to our Github project. See the steps below for doing this: on your device, go to .ssh directory and run: ssh-keygen -t ed25519 -C \" $( git config user.email ) \" -f [ ProjectName ] -N \"\" The [name] is the name that is used to write the public and private key files. I suggest to use the name of your project. This command generates a private and public key using the ed25519 algorithm. Go to your Github project settings > deploy keys and click on add deploy key title = Public key of ACTIONS_DEPLOY_KEY paste the contents of your [ProjectName].pub file into this field. This is your public key that you just generated. Go to Github project settings > Secrets > Actions and click on new repository secret secret name = ACTIONS_DEPLOY_KEY > you set this as title, since you refer to this variable name in your gh-pages.yml file ( deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} ) paste the contents of your [ProjectName] file in the .ssh folder. This is the private key that you just generated. Now you are all set. Now you can push your changes to your Github default branch. If there are no unexpected issues, Github will automatically deploy your changes to the Github Pages site, based on your workflow. Tools & Plugins \u00b6 markdownlint-cli \u00b6 When you have a project with markdown ( .md ) files, or when building documentation sites based on markdown files (e. g., with Mkdocs), you can use markdownlint-cli to improve your markdown code. Install with homebrew: brew install markdownlint-cli It uses a default ruleset to check your files, but you can also tweak that ruleset by including a .markdownlint. yml or .markdownlint.jsonc file in your project's root. In this file you can change the ruleset. See this yml file for an example. This file also explains the rules and their behaviour. template \u00b6 I use the following .markdownlint.yml template: --- default : true # MD003/heading-style/header-style - Heading style MD003 : style : atx # MD004/ul-style - Unordered list style MD004 : style : asterisk # MD007/ul-indent - Unordered list indentation MD007 : indent : 4 # MD013/line-length - Line length MD013 : false # MD025/single-title/single-h1 - Multiple top-level headings # in the same document MD025 : false # MD030/list-marker-space - Spaces after list markers MD030 : ul_multi : 1 ol_multi : 2 # MD035/hr-style - Horizontal rule style MD035 : style : --- # MD046/code-block-style - Code block style MD046 : false pre-commit \u00b6 pre-commit helps you implement checks before committing code. You install it through pip or poetry: poetry add pre-commit . Subsequently, you create a .pre-commit-config.yml file where you include the checks that must be performed. Set up the pre-commit hooks for your project: pre-commit install Now, every time you commit code, the pre-commit hook will check your code based on the checks included in your config file. Manually trigger a pre-commit check: pre-commit run --all-files Plugins \u00b6 There are alot of plugins that you can use when building a doc site with Mkdocs and Material theme. Some of them are listed below. markdown_extensions \u00b6 admonitions \u00b6 Admonitions , also known as call-outs, are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several types of admonitions and allows for the inclusion and nesting of arbitrary content. To enable this extension, add the following to mkdocs.yml : markdown_extensions : - admonition - pymdownx.details - pymdownx.superfences Admonitions follow a simple syntax: a block starts with !!!, followed by a single keyword used as a type qualifier. The content of the block follows on the next line, indented by four spaces. See their site for more details. References \u00b6 See the links below for guides and other docs that helped me understand Mkdocs and deploying to Github Pages: Creating a documentation site with mkdocs Hosting a MkDocs-driven documentation site on GitHub Pages Mkdocs plugins Best of MKDocs Deploying a Mkdocs site with Github Actions","title":"MkDocs"},{"location":"tooling/setup-mkdocs/#getting-started-with-mkdocs","text":"This page explains how to use MkDocs and host it on Github Pages.","title":"Getting started with Mkdocs"},{"location":"tooling/setup-mkdocs/#create-mkdocs-site","text":"pip3 install mkdocs installs the mkdocs package to generate python based static sites. mkdocs new [site-name] creates a new project with the correct set-up. mkdocs serve - Start the live-reloading docs server. mkdocs build subsequently generates your static pages and places the files in /sites . Make sure to include this in your .gitignore file.","title":"Create mkdocs site"},{"location":"tooling/setup-mkdocs/#setting-up-a-python-environment","text":"Here we explain how to set up a virtual environment using pyenv and use requirements.txt to keep track of our dependencies. Instead of using pyenv you can also directly start with Poetry. Further down this guide we explain the steps regarding a Poetry set up. setup a python environment ( pyenv virtualenv [environmentName] ) and activate it with pyenv activate [environmentName] . Install packages and plugins accordingly. Run pip freeze > requirements.txt to output your deps into requirements.txt . Add .venv to your .gitignore file.","title":"Setting up a Python environment"},{"location":"tooling/setup-mkdocs/#host-on-gh-pages","text":"Create a new repository in Github (empty one). Perform git init in your local mkdocs folder (the root folder that stores the mkdocs.yml file). The usual stuff (git add, git commit, git remote add origin [remote-url] and git push -u origin main). Now you need to perform the mkdocs gh-deploy command in your local mkdocs project. This creates a Git branch names gh-pages for Github Pages to pick up your site. Running this command will generate the static website. Rather than outputting the files in the site folder as we saw in the previous post, the website will be saved in a new branch named gh-pages and a push of this branch is done towards GitHub. If you go to the Actions tab on the GitHub repository, you will see GitHub automatically picking up the new gh-pages branch and deploying it to GitHub pages. Now your site is hosted on github pages accordingly. Everything has been set up!","title":"Host on gh pages"},{"location":"tooling/setup-mkdocs/#automated-deployments","text":"So far I have described how to manually deploy your mkdocs site to Github Pages. The next steps explain how you can do this automatically through Github Actions. Go to your Github repo and then to Actions menu. Click on New workflow and select the suggested workflow (simple workflow). This will generate a new file called blank.yml in the .github/workflows directory. See the example below for the changes that we conducted on the template file. I suggest to copy this over. Please be mindful of your default branch's name! If you have master , make sure to reflect that in the yaml file below as well. --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt - name : Build site run : mkdocs gh-deploy --force --clean --verbose Change the file name to gh-pages.yml and make sure to commit this change to the default branch. Remember that the gh-pages branch will be generated by MkDocs, and we never want to either check out this branch locally or modify it manually. Now you have a minimal set-up for deploying your site through Github Actions automatically. The next steps refines our Github Workflow approach. These steps are not really needed if you don't want to. You can continue using pip3 with pyenv (or venv) for dependency and environment management, instead of poetry. You may also decide to add a Deploy job without integrating with Poetry all together. You simply change the yml file as follows: # I will not include the stuff that won't be changed - name : Build site run : mkdocs build --verbose # here I have updated the code from gh-deploy to build # When you replace gh-deploy, you need to add the step below in order to push the site to gh-pages: - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} # ----- SEE NEXT SECTION FOR DEPLOYMENT KEYS! ----- publish_dir : ./site","title":"Automated deployments"},{"location":"tooling/setup-mkdocs/#refining-github-workflow","text":"After our first deployment, we do a git fetch and git pull in our local project. This will update our local folder and adds the newly created contents inside the .github directory. Before we use Poetry locally, we first need to make sure that we even have a Poetry environment and pyproject. toml file in our root directory. We run poetry init , and install the dependencies that are listed in our requirements.txt file. Now we tweak the gh-pages.yml file as follows: --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install Poetry uses : snok/install-poetry@v1 with : virtualenvs-create : true virtualenvs-in-project : true installer-parallel : true - name : Load cached venv id : cached-poetry-dependencies uses : actions/cache@v3 with : path : .venv key : venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }} - name : Install dependencies if cache doesnt exist if : steps.cached-poetry-dependencies.outputs.cache-hit != 'true' run : poetry install --no-interaction --no-root - name : Install project run : poetry install --no-interaction --no-root - name : Make the site run : poetry run mkdocs build --verbose # run: poetry run mkdocs gh-deploy --force --clean --verbose - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./site As shown above, we add a couple of jobs that take care of installing Poetry and our project dependencies. We also replace our run command in the Make the site job: we add poetry but also replace gh-deploy with build . Again, you may choose to keep poetry run mkdocs gh-deploy . Doing so, renders the job Deploy redundant. I suggest to remove that job accordingly. Since we are newly introducing Poetry to our pipeline, it might be good to first test if this is integrated correctly, and we don't experience any issues. If everything is fine, we can work on the next step.","title":"Refining Github Workflow"},{"location":"tooling/setup-mkdocs/#deploy-job","text":"Now that we don't use the gh-deploy functionality provided by MkDocs , we need to add another job that takes care of pushing our site to the gh-pages branch. That's where the Deploy job comes in. You'll notice the following parameter: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} . Because we don't make use of the gh-deploy functionality, we need to make use of Github Runners : agents that are used to deploy code to certain environments. In our case, it is the gh-pages environment (i.e., branch). However, we need to properly authenticate the Runner otherwise it will not be able to perform write operations, thus deploying our site. To ensure that the Runner is able to authenticate, we make use of deploy keys and add them to our Github project. See the steps below for doing this: on your device, go to .ssh directory and run: ssh-keygen -t ed25519 -C \" $( git config user.email ) \" -f [ ProjectName ] -N \"\" The [name] is the name that is used to write the public and private key files. I suggest to use the name of your project. This command generates a private and public key using the ed25519 algorithm. Go to your Github project settings > deploy keys and click on add deploy key title = Public key of ACTIONS_DEPLOY_KEY paste the contents of your [ProjectName].pub file into this field. This is your public key that you just generated. Go to Github project settings > Secrets > Actions and click on new repository secret secret name = ACTIONS_DEPLOY_KEY > you set this as title, since you refer to this variable name in your gh-pages.yml file ( deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} ) paste the contents of your [ProjectName] file in the .ssh folder. This is the private key that you just generated. Now you are all set. Now you can push your changes to your Github default branch. If there are no unexpected issues, Github will automatically deploy your changes to the Github Pages site, based on your workflow.","title":"Deploy job"},{"location":"tooling/setup-mkdocs/#tools-plugins","text":"","title":"Tools &amp; Plugins"},{"location":"tooling/setup-mkdocs/#markdownlint-cli","text":"When you have a project with markdown ( .md ) files, or when building documentation sites based on markdown files (e. g., with Mkdocs), you can use markdownlint-cli to improve your markdown code. Install with homebrew: brew install markdownlint-cli It uses a default ruleset to check your files, but you can also tweak that ruleset by including a .markdownlint. yml or .markdownlint.jsonc file in your project's root. In this file you can change the ruleset. See this yml file for an example. This file also explains the rules and their behaviour.","title":"markdownlint-cli"},{"location":"tooling/setup-mkdocs/#template","text":"I use the following .markdownlint.yml template: --- default : true # MD003/heading-style/header-style - Heading style MD003 : style : atx # MD004/ul-style - Unordered list style MD004 : style : asterisk # MD007/ul-indent - Unordered list indentation MD007 : indent : 4 # MD013/line-length - Line length MD013 : false # MD025/single-title/single-h1 - Multiple top-level headings # in the same document MD025 : false # MD030/list-marker-space - Spaces after list markers MD030 : ul_multi : 1 ol_multi : 2 # MD035/hr-style - Horizontal rule style MD035 : style : --- # MD046/code-block-style - Code block style MD046 : false","title":"template"},{"location":"tooling/setup-mkdocs/#pre-commit","text":"pre-commit helps you implement checks before committing code. You install it through pip or poetry: poetry add pre-commit . Subsequently, you create a .pre-commit-config.yml file where you include the checks that must be performed. Set up the pre-commit hooks for your project: pre-commit install Now, every time you commit code, the pre-commit hook will check your code based on the checks included in your config file. Manually trigger a pre-commit check: pre-commit run --all-files","title":"pre-commit"},{"location":"tooling/setup-mkdocs/#plugins","text":"There are alot of plugins that you can use when building a doc site with Mkdocs and Material theme. Some of them are listed below.","title":"Plugins"},{"location":"tooling/setup-mkdocs/#markdown_extensions","text":"","title":"markdown_extensions"},{"location":"tooling/setup-mkdocs/#admonitions","text":"Admonitions , also known as call-outs, are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several types of admonitions and allows for the inclusion and nesting of arbitrary content. To enable this extension, add the following to mkdocs.yml : markdown_extensions : - admonition - pymdownx.details - pymdownx.superfences Admonitions follow a simple syntax: a block starts with !!!, followed by a single keyword used as a type qualifier. The content of the block follows on the next line, indented by four spaces. See their site for more details.","title":"admonitions"},{"location":"tooling/setup-mkdocs/#references","text":"See the links below for guides and other docs that helped me understand Mkdocs and deploying to Github Pages: Creating a documentation site with mkdocs Hosting a MkDocs-driven documentation site on GitHub Pages Mkdocs plugins Best of MKDocs Deploying a Mkdocs site with Github Actions","title":"References"}]}