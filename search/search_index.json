{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 This is my digital garden . This is where I write. Keep track of my thoughts, be it lingering or active ones. Write down ideas, concepts and topics that interest me or that I might need in the future. In the past I've tried to take notes in a notebook, on my ipad, but also using third party applications such as Obsidian and Notion . Alas, as time passes, I find myself incapable of actively maintaining either one of them, or extract meaningful and longterm value from them. The personal notebook or Ipad notes become cluttered and unorganized, losing my motivation to use it as a personal library. Similar feelings arose when using Notion.so. Building comprehensive tables, code snippets, and referring to other pages often became a mess as well. I found that Obisidan grew on me - I even purschased a yearly subscription to support the developers. It has a steep learning curve, but eventually I got the hang of it. Unfortunately, this become somewhat difficult for me to maintain as well: it is self hosted on my laptop, which I don't all the time. Instead, I often work from devices that are provided by my clients, making it difficult to actively work in Obsidian. Additionally, I can't seem to use Obsidian for light-hearted topics such as fitness, or use it to put in my fleeting notes. I can't shake the feeling that I need to use Obsidian for academic grade documentation only, or use it to document advanced topics. This ultimately led me back to old school note-taking, scattering my thoughts all over the place (Notion, Obsidian, physical notebooks, and Apple Notes). With digital garden, I embark on my next journey of collecting and maintaining my thoughts; a backup of my brains, so to speak. As Gwern put it: The goal of these pages is not to be a model of concision, maximizing entertainment value per word, or to preach to a choir by elegantly repeating a conclusion. Rather, I am attempting to explain things to my future self, who is intelligent and interested, but has forgotten. What I am doing is explaining why I decided what I did to myself and noting down everything I found interesting about it for future reference. I hope my other readers, whomever they may be, might find the topic as interesting as I found it, and the essay useful or at least entertaining\u2013but the intended audience is my future self. Principles \u00b6 Based on Gwern's long site principle . Heavily inspired by Lyz Digital Garden .","title":"Introduction"},{"location":"#introduction","text":"This is my digital garden . This is where I write. Keep track of my thoughts, be it lingering or active ones. Write down ideas, concepts and topics that interest me or that I might need in the future. In the past I've tried to take notes in a notebook, on my ipad, but also using third party applications such as Obsidian and Notion . Alas, as time passes, I find myself incapable of actively maintaining either one of them, or extract meaningful and longterm value from them. The personal notebook or Ipad notes become cluttered and unorganized, losing my motivation to use it as a personal library. Similar feelings arose when using Notion.so. Building comprehensive tables, code snippets, and referring to other pages often became a mess as well. I found that Obisidan grew on me - I even purschased a yearly subscription to support the developers. It has a steep learning curve, but eventually I got the hang of it. Unfortunately, this become somewhat difficult for me to maintain as well: it is self hosted on my laptop, which I don't all the time. Instead, I often work from devices that are provided by my clients, making it difficult to actively work in Obsidian. Additionally, I can't seem to use Obsidian for light-hearted topics such as fitness, or use it to put in my fleeting notes. I can't shake the feeling that I need to use Obsidian for academic grade documentation only, or use it to document advanced topics. This ultimately led me back to old school note-taking, scattering my thoughts all over the place (Notion, Obsidian, physical notebooks, and Apple Notes). With digital garden, I embark on my next journey of collecting and maintaining my thoughts; a backup of my brains, so to speak. As Gwern put it: The goal of these pages is not to be a model of concision, maximizing entertainment value per word, or to preach to a choir by elegantly repeating a conclusion. Rather, I am attempting to explain things to my future self, who is intelligent and interested, but has forgotten. What I am doing is explaining why I decided what I did to myself and noting down everything I found interesting about it for future reference. I hope my other readers, whomever they may be, might find the topic as interesting as I found it, and the essay useful or at least entertaining\u2013but the intended audience is my future self.","title":"Introduction"},{"location":"#principles","text":"Based on Gwern's long site principle . Heavily inspired by Lyz Digital Garden .","title":"Principles"},{"location":"coding/intellij-idea/","text":"Although IntelliJ IDEA is tailored for Java and JVM, it is a multi-language IDE that supports Rust, Javascript, Python, SQL, and numerous other languages. Unicode strings for apple symbols: Sym Key \u2303 Control \u2325 Option \u21e7 Shift \u2318 Command Navigation \u00b6 Action Description \u2303 + [ Move caret backwards a paragraph \u2303 + ] Move caret forwards a paragraph \u2318 + l Jump to line:column \u2318 + e Go to recent files \u2318 + \u21e7 + a Opens up actions \u2325 + enter Opens up suggestions pane (yellow lightbulb) options for the code where you caret is \u2318 + 1 Go to Project sidebar \u2325 + f12 Go to terminal","title":"Intellij IDEA"},{"location":"coding/intellij-idea/#navigation","text":"Action Description \u2303 + [ Move caret backwards a paragraph \u2303 + ] Move caret forwards a paragraph \u2318 + l Jump to line:column \u2318 + e Go to recent files \u2318 + \u21e7 + a Opens up actions \u2325 + enter Opens up suggestions pane (yellow lightbulb) options for the code where you caret is \u2318 + 1 Go to Project sidebar \u2325 + f12 Go to terminal","title":"Navigation"},{"location":"coding/architecture/ddd/","text":"Domain Driven Design \u00b6 Nice reddit post discussing domain driven design. Another interesting post is provided by microsoft where they talk about ddd and Command and Query Responsibility Segregation (CQRS).","title":"Domain Driven Design"},{"location":"coding/architecture/ddd/#domain-driven-design","text":"Nice reddit post discussing domain driven design. Another interesting post is provided by microsoft where they talk about ddd and Command and Query Responsibility Segregation (CQRS).","title":"Domain Driven Design"},{"location":"coding/architecture/repository-pattern/","text":"This section draws heavily from Martin Fowler's Patterns of Enterprise Architecture Applications, and Domain Driven Design . A system with a complex domain model (the place where the business logic is put) often benefits from a layer, such as the one provided by Data Mapper , that isolates domain objects from details of the database access code. In such systems it can be worthwhile to build another layer of abstraction over the mapping layer where query construction code is concentrated. This becomes more important when there are a large number of domain classes or heavy querying. In these cases particularly, adding this layer helps minimize duplicate query logic. A Repository mediates between the domain and data mapping layers, acting like an in-memory domain object collection. Client objects construct query specifications declaratively and submit them to Repository for satisfaction. Objects can be added to and removed from the Repository, as they can from a simple collection of objects, and the mapping code encapsulated by the Repository will carry out the appropriate operations behind the scenes. Conceptually, a Repository encapsulates the set of objects persisted in a data store and the operations performed over them, providing a more object-oriented view of the persistence layer. Repository also supports the objective of achieving a clean separation and one-way dependency between the domain and data mapping layers. Info A repository can be considered an interface that serves as your apps's gateway to the database, in terms of object-relational mappings.","title":"Repository pattern"},{"location":"coding/python/","text":"Layout \u00b6 I am actively using Python for several use case: transform and load complex Excel files. web scrawling (e.g., Funda scraper). web application for IT Risk Management (based on FastAPI). My Python docs will focus on concepts that I have encountered throughout the development of my use cases, in contrast to covering topics that I have learned throughout several courses (e.g., bootcamp courses on Udemy). The reasoning behind this, is that focus on topics that I encountered will resonate better with me; after all, I have used or needed them when building stuff. Developer roadmap \u00b6 There is a good Python developer roadmap on Reddit if you are new to Python (it can be found on Github as well). The roadmap covers the following concepts: Data structures Data management Data flows OOP Language skeleton Multithreading & Multiprocessing Common practices Algorithms Databases Web Architecture References \u00b6 Hackers & Slackers provides alot of material related to Python. Examples include: working with Excel, SQLAlchemy, GraphQL, Web Scraping, SQL native, and more.","title":"Python"},{"location":"coding/python/#layout","text":"I am actively using Python for several use case: transform and load complex Excel files. web scrawling (e.g., Funda scraper). web application for IT Risk Management (based on FastAPI). My Python docs will focus on concepts that I have encountered throughout the development of my use cases, in contrast to covering topics that I have learned throughout several courses (e.g., bootcamp courses on Udemy). The reasoning behind this, is that focus on topics that I encountered will resonate better with me; after all, I have used or needed them when building stuff.","title":"Layout"},{"location":"coding/python/#developer-roadmap","text":"There is a good Python developer roadmap on Reddit if you are new to Python (it can be found on Github as well). The roadmap covers the following concepts: Data structures Data management Data flows OOP Language skeleton Multithreading & Multiprocessing Common practices Algorithms Databases Web Architecture","title":"Developer roadmap"},{"location":"coding/python/#references","text":"Hackers & Slackers provides alot of material related to Python. Examples include: working with Excel, SQLAlchemy, GraphQL, Web Scraping, SQL native, and more.","title":"References"},{"location":"coding/python/data-structures/","text":"Everything in Python is an object. Each object has its own data attributes and methods associated with it. In order to use an object efficiently and appropriately, we should know how to interact with them. Lists, tuples, and sets are 3 important types of objects. What they have in common is that they are used as data structures. In order to create robust and well-performing products, one must know the data structures of a programming language very well. Data structures that I use the most are: list tuple dict set array & bytes range dataclass Lists, tuples & sets \u00b6 list is a built-in data structure in Python. It is represented as a collection of data points in square brackets. Lists can be used to store any data type or a mixture of different data types. Lists are mutable which is one of the reasons why they are so commonly used. tuple is a collection of values separated by comma and enclosed in parentheses. Unlike lists, tuples are immutable. The immutability can be considered as the identifying feature of tuples. set is an unordered collection of distinct immutable objects. A set contains unique elements. Although sets are mutable, the elements of sets must be immutable. There is no order associated with the elements of a set. Thus, it does not support indexing or slicing like we do with lists. my_list = [ 1 , 2 , \"3\" ] my_set = { 1 , 2 , \"3\" } my_tuple = ( 1 , 2 , \"3\" ) List vs Sets \u00b6 Consider the following: text = \"Hello World!\" print ( list ( text )) [ 'H' , 'e' , 'l' , 'l' , 'o' , ' ' , 'W' , 'o' , 'r' , 'l' , 'd' , '!' ] print ( set ( text )) { 'H' , 'W' , 'o' , ' ' , 'l' , 'r' , '!' , 'e' , 'd' } As you can see above, the first print statement returns a list (i) with all the characters in the text variable and (ii) ordered based on the order of the chars in the text string. The second statement returns a set (i) containing only unique characters, as a set doesn't allow duplicate values. Secondly, (ii) a set is unordered, hence the random positions of the chars in the set. In the previous example, we saw that sets do not possess an order. Thus, we cannot do slicing or indexing on sets like we do with lists. The example below demonstrates this: text = \"Hello World!\" list_a = list ( text ) print ( list_a [: 2 ]) # print item in list from index position 0 up until (but excluding) 2 [ 'H' , 'e' ] set_a = set ( text ) print ( set_a [: 2 ]) # returns a TypeError: 'set' object is not subscriptable List vs Tuples \u00b6 The difference between list and tuple is the mutability. Unlike lists, tuples are immutable. For instance, we can add items to a list but cannot do it with tuples. The methods that change a collection (e.g. append, remove, extend, pop) are not applicable to tuples, as shown below: list_a : list [ int ] = [ 1 , 2 , 3 , 4 ] # apply type specification list_a . append ( 5 ) print ( list_a ) [ 1 , 2 , 3 , 4 , 5 ] tuple_a = ( 1 , 2 , 3 , 4 ) tuple_a . append ( 5 ) # AttributeError: 'tuple' object has no attribute 'append' The immutability might be the most identifying feature of tuples. We cannot (re)assign a value to an item of a tuple: tuple_a = ( 3 , 5 , 'x' , 5 ) tuple_a [ 0 ] = 7 # will generate an error Although tuples are immutable, they can contain mutable elements such as lists or sets: tuple_a = ([ 1 , 3 ], 'a' , 'b' , 8 ) # this tuple contains a list with 2 items tuple_a [ 0 ][ 0 ] = 99 # we reassign our first item in the list from value 1 to 99 print ( tuple_a ) ([ 99 , 3 ], 'a' , 'b' , 8 ) Working with lists, sets, and tuples \u00b6 References \u00b6 15 examples to master Python lists, sets and tuples Difference between del, remove, and pop *","title":"Data structures"},{"location":"coding/python/data-structures/#lists-tuples-sets","text":"list is a built-in data structure in Python. It is represented as a collection of data points in square brackets. Lists can be used to store any data type or a mixture of different data types. Lists are mutable which is one of the reasons why they are so commonly used. tuple is a collection of values separated by comma and enclosed in parentheses. Unlike lists, tuples are immutable. The immutability can be considered as the identifying feature of tuples. set is an unordered collection of distinct immutable objects. A set contains unique elements. Although sets are mutable, the elements of sets must be immutable. There is no order associated with the elements of a set. Thus, it does not support indexing or slicing like we do with lists. my_list = [ 1 , 2 , \"3\" ] my_set = { 1 , 2 , \"3\" } my_tuple = ( 1 , 2 , \"3\" )","title":"Lists, tuples &amp; sets"},{"location":"coding/python/data-structures/#list-vs-sets","text":"Consider the following: text = \"Hello World!\" print ( list ( text )) [ 'H' , 'e' , 'l' , 'l' , 'o' , ' ' , 'W' , 'o' , 'r' , 'l' , 'd' , '!' ] print ( set ( text )) { 'H' , 'W' , 'o' , ' ' , 'l' , 'r' , '!' , 'e' , 'd' } As you can see above, the first print statement returns a list (i) with all the characters in the text variable and (ii) ordered based on the order of the chars in the text string. The second statement returns a set (i) containing only unique characters, as a set doesn't allow duplicate values. Secondly, (ii) a set is unordered, hence the random positions of the chars in the set. In the previous example, we saw that sets do not possess an order. Thus, we cannot do slicing or indexing on sets like we do with lists. The example below demonstrates this: text = \"Hello World!\" list_a = list ( text ) print ( list_a [: 2 ]) # print item in list from index position 0 up until (but excluding) 2 [ 'H' , 'e' ] set_a = set ( text ) print ( set_a [: 2 ]) # returns a TypeError: 'set' object is not subscriptable","title":"List vs Sets"},{"location":"coding/python/data-structures/#list-vs-tuples","text":"The difference between list and tuple is the mutability. Unlike lists, tuples are immutable. For instance, we can add items to a list but cannot do it with tuples. The methods that change a collection (e.g. append, remove, extend, pop) are not applicable to tuples, as shown below: list_a : list [ int ] = [ 1 , 2 , 3 , 4 ] # apply type specification list_a . append ( 5 ) print ( list_a ) [ 1 , 2 , 3 , 4 , 5 ] tuple_a = ( 1 , 2 , 3 , 4 ) tuple_a . append ( 5 ) # AttributeError: 'tuple' object has no attribute 'append' The immutability might be the most identifying feature of tuples. We cannot (re)assign a value to an item of a tuple: tuple_a = ( 3 , 5 , 'x' , 5 ) tuple_a [ 0 ] = 7 # will generate an error Although tuples are immutable, they can contain mutable elements such as lists or sets: tuple_a = ([ 1 , 3 ], 'a' , 'b' , 8 ) # this tuple contains a list with 2 items tuple_a [ 0 ][ 0 ] = 99 # we reassign our first item in the list from value 1 to 99 print ( tuple_a ) ([ 99 , 3 ], 'a' , 'b' , 8 )","title":"List vs Tuples"},{"location":"coding/python/data-structures/#working-with-lists-sets-and-tuples","text":"","title":"Working with lists, sets, and tuples"},{"location":"coding/python/data-structures/#references","text":"15 examples to master Python lists, sets and tuples Difference between del, remove, and pop *","title":"References"},{"location":"coding/python/libraries/sqlalchemy/","tags":["db"],"text":"SQLAlchemy is an ORM. See the library's author post on the design principles behind it. For instance, it is designed with keeping the Repository pattern in mind. Introducing SQLAlchemy \u00b6 SQLAlchemy (SA hereafter) is a library that helps you interact with databases in python. It can be used to perform ad hoc querying on your database (db), but also used as an ORM for larger applications. However you use it, the engine object is necessary in order to initiate a connection between your code and the database. You create it once, up front, by passing in the db's connection URI. The engine acts as your central source of connections to your database. This engine is initiated using create_engine() . You pass in the db URI and other, optional, arguments (e.g., we used echo=True during our dev work, as it shows every action on your db). After creating the engine, you can already start interacting with the database, for instance by executing the engine.execute() method. This method enables you to pass SQL queries to the database and perform ad hoc SQL querying. Be careful when interacting with the engine directly When you do decide to interact with the engine directly, make sure to wrap any SQL query in the text() method provided by SQLAlchemy. This escapes dangerous characters found in queries that may break your db. Hackers & Slackers made a good post on the basics of SA and interacting with the engine directly and parsing the results. For instance, the post describes how you can parse results as a list of dictionaries using: from sqlalchemy import create_engine , text # noqa # create engine engine = create_engine ( \"mysql+pymysql://user:password@host:3306/database\" , echo = True , ) # perform ad hoc query and assign to 'results' object results = engine . execute ( text ( \"SELECT job_id, agency, business_title, \\ salary_range_from, salary_range_to \\ FROM nyc_jobs ORDER BY RAND() LIMIT 10;\" ) ) # contents of results are converting to a list. Each db entry will be a dictionary. # The list will be assigned to the rows variable. rows = [ dict ( row ) for row in results . fetchall ()] # the result is printed print ( rows ) SQLAlchemy's ORM module \u00b6 Interacting directly with the engine might be okay for simple tasks, but when you are building an application is might be better to start with the ORM from the get-go. By using the ORM, you basically abstract away the SQL stuff. You interact with the db using Model objects, i.e. data classes. In this case, Models represent your db's tables, the attributes of Models represent the table's columns. Declarative mapping \u00b6 To enable us to create tables and columns from our data classes, we eed to construct a mapping between a class (i.e., Model) and the table (and it's columns). This concept is called declarative mapping . The most common pattern is to construct a base class, which will apply the declarative mapping process to all subclasses that are derived from the baseclass. SA provides the declarative_base() function to do just that. Example of using it: from sqlalchemy import declarative_base # noqa Base = declarative_base () The new base class will be given a metaclass that produces appropriate Table objects and makes the appropriate mapper() calls based on the information provided declaratively in the class and any subclasses of the class. For more on mapping patterns supported by SA, see their docs . Data Mapper pattern & how SA applies it Data Mapper - The key to this pattern is that object-relational mapping is applied to a user-defined class in a transparent way, keeping the details of persistence separate from the public interface of the class. SQLAlchemy's classical mapping system, which is the usage of the mapper() function to link a class with table metadata, implemented this pattern as fully as possible. In modern SQLAlchemy, we use the Declarative pattern which combines table metadata with the class' declaration as a shortcut to using mapper(), but the persistence API remains separate. Models \u00b6 The User class example that is shown below, is a Model that is used to represent a users table in our database. You can see that we have imported several SQLAlchemy types , which we see getting passed into each Column . Each type corresponds to a SQL data type. Thus, our SQL table columns' data types would be integer , varchar(255) , and text , respectively. Columns can also accept optional parameters to things like keys or column constraints: primary_key: Designates a column as the table's \"primary key,\" a highly recommended practice that serves as a unique identifier as well as an index for SQL to search on. autoincrement: Only relevant to columns that are both the primary_key and have the type Integer . Each user we create will automatically be assigned an id , where our first user will have an id of 1, and subsequent users would increment accordingly. unique: Places a constraint where no two records/rows share the same value for the given column (we don't want two users to have the same username). nullable: When set to True , adds a constraint that the column is mandatory, and no row will be created unless a value is provided. key: Places a secondary key on the given column, typically used in tandem with another constraint such as index . index: Designates that a column's values are sortable in a non-arbitrary way in the interest of improving query performance. server_default: A default value to assign if a value is not explicitly passed. Specifying tablename In our example, we set the optional attribute tablename to explictly specify what model's corresponding SQL table should be named. When not present, SQL will use the name of the class to create the table. Next to specifying our attributes and associated columns, and tablename , we can also define a __repr__ method: it's best practice to set the value of repr on data models (and Python classes in general) for the purpose of logging or debugging our class instances. The value returned by repr is what we'll see when we print() an instance of User. If you've ever had to deal with [object Object] in Javascript, you're already familiar with how obnoxious it is to debug an object's value and receive nothing useful in return. See example below: \"\"\"models.py\"\"\" from sqlalchemy import declarative_base , Column , Integer , Text , String , DateTime # noqa from sqlalchemy.sql import func # noqa Base = declarative_base () class User ( Base ): \"\"\"User account.\"\"\" __tablename__ = \"user\" id = Column ( Integer , primary_key = True , autoincrement = \"auto\" ) username = Column ( String ( 255 ), unique = True , nullable = False ) password = Column ( Text , nullable = False ) email = Column ( String ( 255 ), unique = True , nullable = False ) def __repr__ ( self ): return f \"<User { self . username } >\" Now, when you create an instance of a model (e.g., you create a user), you are effectively creating a new row in the associated db table. But before you can do that, you first need to make sure that you have a table already. Now, you can either create that by accessing your database and create a table manually, but that would be quite inefficient. Luckily SA provides a method on your declarative_base() class that creates the tables for you: # continues from above Base . metadata . create_all ( engine ) # noqa Once that runs, SQLAlchemy handles everything on the database side to create a table matching our model. Creating a session \u00b6 A session is a persistent database connection that makes it easier to communicate with our database. The session begins in a mostly stateless form; once the session object is called, it requests a connection resource from the engine that it is bound to. Sessions are created by binding them to an SQLAlchemy engine, which we covered in the previous part. With an engine created, all we need is to use SQLAlchemy's sessionmaker to define a session and bind it to our engine: \"\"\"database.py\"\"\" \"\"\"Database engine & session creation.\"\"\" from sqlalchemy import create_engine # noqa from sqlalchemy.orm import sessionmaker # noqa engine = create_engine ( 'mysql+pymysql://user:password@host:3600/database' , echo = True ) Session = sessionmaker ( bind = engine ) session = Session () On sessionmaker \u00b6 Session is a regular Python class which can be directly instantiated. However, to standardize how sessions are configured and acquired, the sessionmaker class is normally used to create a top level Session configuration which can then be used throughout an application without the need to repeat the configuration arguments. Storing data using Models & Sessions \u00b6 With a model defined and session created, we have the luxury of adding and modifying data purely in Python. SQLAlchemy refers to this as function-based query construction . To create a record in our users db table, we need to create an instance of our User model, and use our session to pass our instance to the engine: from models import User # noqa from database import session # noqa user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) session . add ( user ) # add the user session . commit () # commit the change With an instance of User created, all it takes to create this user in our database are two calls to our session: add() queues the item for creation, and commit() saves the change. We should now see a row in our database's user table! Working with session is as easy as four simple methods: session.add() : We can pass an instance of a data model into add() to quickly create a new record to be added to our database. session.delete() : Like the above, delete() accepts an instance of a data model. If that record exists in our database, it will be staged for deletion. session.commit() : Changes made within a session are not saved until explicitly committed. session.close() : Unlike SQLAlchemy engines, sessions are connections that remain open until explicitly closed. References \u00b6 Check out the following references that I've used to increase my understanding of SA: Databases in Python made easy with SQLAlchemy is a 4-part introduction to SQLAlchemy. It's from 2019, so some stuff might be outdated. Overall though, the tutorial helped me understand the syntax and workings of SA better.","title":"SQLAlchemy"},{"location":"coding/python/libraries/sqlalchemy/#introducing-sqlalchemy","text":"SQLAlchemy (SA hereafter) is a library that helps you interact with databases in python. It can be used to perform ad hoc querying on your database (db), but also used as an ORM for larger applications. However you use it, the engine object is necessary in order to initiate a connection between your code and the database. You create it once, up front, by passing in the db's connection URI. The engine acts as your central source of connections to your database. This engine is initiated using create_engine() . You pass in the db URI and other, optional, arguments (e.g., we used echo=True during our dev work, as it shows every action on your db). After creating the engine, you can already start interacting with the database, for instance by executing the engine.execute() method. This method enables you to pass SQL queries to the database and perform ad hoc SQL querying. Be careful when interacting with the engine directly When you do decide to interact with the engine directly, make sure to wrap any SQL query in the text() method provided by SQLAlchemy. This escapes dangerous characters found in queries that may break your db. Hackers & Slackers made a good post on the basics of SA and interacting with the engine directly and parsing the results. For instance, the post describes how you can parse results as a list of dictionaries using: from sqlalchemy import create_engine , text # noqa # create engine engine = create_engine ( \"mysql+pymysql://user:password@host:3306/database\" , echo = True , ) # perform ad hoc query and assign to 'results' object results = engine . execute ( text ( \"SELECT job_id, agency, business_title, \\ salary_range_from, salary_range_to \\ FROM nyc_jobs ORDER BY RAND() LIMIT 10;\" ) ) # contents of results are converting to a list. Each db entry will be a dictionary. # The list will be assigned to the rows variable. rows = [ dict ( row ) for row in results . fetchall ()] # the result is printed print ( rows )","title":"Introducing SQLAlchemy"},{"location":"coding/python/libraries/sqlalchemy/#sqlalchemys-orm-module","text":"Interacting directly with the engine might be okay for simple tasks, but when you are building an application is might be better to start with the ORM from the get-go. By using the ORM, you basically abstract away the SQL stuff. You interact with the db using Model objects, i.e. data classes. In this case, Models represent your db's tables, the attributes of Models represent the table's columns.","title":"SQLAlchemy's ORM module"},{"location":"coding/python/libraries/sqlalchemy/#declarative-mapping","text":"To enable us to create tables and columns from our data classes, we eed to construct a mapping between a class (i.e., Model) and the table (and it's columns). This concept is called declarative mapping . The most common pattern is to construct a base class, which will apply the declarative mapping process to all subclasses that are derived from the baseclass. SA provides the declarative_base() function to do just that. Example of using it: from sqlalchemy import declarative_base # noqa Base = declarative_base () The new base class will be given a metaclass that produces appropriate Table objects and makes the appropriate mapper() calls based on the information provided declaratively in the class and any subclasses of the class. For more on mapping patterns supported by SA, see their docs . Data Mapper pattern & how SA applies it Data Mapper - The key to this pattern is that object-relational mapping is applied to a user-defined class in a transparent way, keeping the details of persistence separate from the public interface of the class. SQLAlchemy's classical mapping system, which is the usage of the mapper() function to link a class with table metadata, implemented this pattern as fully as possible. In modern SQLAlchemy, we use the Declarative pattern which combines table metadata with the class' declaration as a shortcut to using mapper(), but the persistence API remains separate.","title":"Declarative mapping"},{"location":"coding/python/libraries/sqlalchemy/#models","text":"The User class example that is shown below, is a Model that is used to represent a users table in our database. You can see that we have imported several SQLAlchemy types , which we see getting passed into each Column . Each type corresponds to a SQL data type. Thus, our SQL table columns' data types would be integer , varchar(255) , and text , respectively. Columns can also accept optional parameters to things like keys or column constraints: primary_key: Designates a column as the table's \"primary key,\" a highly recommended practice that serves as a unique identifier as well as an index for SQL to search on. autoincrement: Only relevant to columns that are both the primary_key and have the type Integer . Each user we create will automatically be assigned an id , where our first user will have an id of 1, and subsequent users would increment accordingly. unique: Places a constraint where no two records/rows share the same value for the given column (we don't want two users to have the same username). nullable: When set to True , adds a constraint that the column is mandatory, and no row will be created unless a value is provided. key: Places a secondary key on the given column, typically used in tandem with another constraint such as index . index: Designates that a column's values are sortable in a non-arbitrary way in the interest of improving query performance. server_default: A default value to assign if a value is not explicitly passed. Specifying tablename In our example, we set the optional attribute tablename to explictly specify what model's corresponding SQL table should be named. When not present, SQL will use the name of the class to create the table. Next to specifying our attributes and associated columns, and tablename , we can also define a __repr__ method: it's best practice to set the value of repr on data models (and Python classes in general) for the purpose of logging or debugging our class instances. The value returned by repr is what we'll see when we print() an instance of User. If you've ever had to deal with [object Object] in Javascript, you're already familiar with how obnoxious it is to debug an object's value and receive nothing useful in return. See example below: \"\"\"models.py\"\"\" from sqlalchemy import declarative_base , Column , Integer , Text , String , DateTime # noqa from sqlalchemy.sql import func # noqa Base = declarative_base () class User ( Base ): \"\"\"User account.\"\"\" __tablename__ = \"user\" id = Column ( Integer , primary_key = True , autoincrement = \"auto\" ) username = Column ( String ( 255 ), unique = True , nullable = False ) password = Column ( Text , nullable = False ) email = Column ( String ( 255 ), unique = True , nullable = False ) def __repr__ ( self ): return f \"<User { self . username } >\" Now, when you create an instance of a model (e.g., you create a user), you are effectively creating a new row in the associated db table. But before you can do that, you first need to make sure that you have a table already. Now, you can either create that by accessing your database and create a table manually, but that would be quite inefficient. Luckily SA provides a method on your declarative_base() class that creates the tables for you: # continues from above Base . metadata . create_all ( engine ) # noqa Once that runs, SQLAlchemy handles everything on the database side to create a table matching our model.","title":"Models"},{"location":"coding/python/libraries/sqlalchemy/#creating-a-session","text":"A session is a persistent database connection that makes it easier to communicate with our database. The session begins in a mostly stateless form; once the session object is called, it requests a connection resource from the engine that it is bound to. Sessions are created by binding them to an SQLAlchemy engine, which we covered in the previous part. With an engine created, all we need is to use SQLAlchemy's sessionmaker to define a session and bind it to our engine: \"\"\"database.py\"\"\" \"\"\"Database engine & session creation.\"\"\" from sqlalchemy import create_engine # noqa from sqlalchemy.orm import sessionmaker # noqa engine = create_engine ( 'mysql+pymysql://user:password@host:3600/database' , echo = True ) Session = sessionmaker ( bind = engine ) session = Session ()","title":"Creating a session"},{"location":"coding/python/libraries/sqlalchemy/#on-sessionmaker","text":"Session is a regular Python class which can be directly instantiated. However, to standardize how sessions are configured and acquired, the sessionmaker class is normally used to create a top level Session configuration which can then be used throughout an application without the need to repeat the configuration arguments.","title":"On sessionmaker"},{"location":"coding/python/libraries/sqlalchemy/#storing-data-using-models-sessions","text":"With a model defined and session created, we have the luxury of adding and modifying data purely in Python. SQLAlchemy refers to this as function-based query construction . To create a record in our users db table, we need to create an instance of our User model, and use our session to pass our instance to the engine: from models import User # noqa from database import session # noqa user = User ( username = \"Bob-12\" , password = \"Please don't set passwords like this\" , email = \"Bob12@example.com\" , ) session . add ( user ) # add the user session . commit () # commit the change With an instance of User created, all it takes to create this user in our database are two calls to our session: add() queues the item for creation, and commit() saves the change. We should now see a row in our database's user table! Working with session is as easy as four simple methods: session.add() : We can pass an instance of a data model into add() to quickly create a new record to be added to our database. session.delete() : Like the above, delete() accepts an instance of a data model. If that record exists in our database, it will be staged for deletion. session.commit() : Changes made within a session are not saved until explicitly committed. session.close() : Unlike SQLAlchemy engines, sessions are connections that remain open until explicitly closed.","title":"Storing data using Models &amp; Sessions"},{"location":"coding/python/libraries/sqlalchemy/#references","text":"Check out the following references that I've used to increase my understanding of SA: Databases in Python made easy with SQLAlchemy is a 4-part introduction to SQLAlchemy. It's from 2019, so some stuff might be outdated. Overall though, the tutorial helped me understand the syntax and workings of SA better.","title":"References"},{"location":"devops/","text":"All things ops related. From editor stuff to postgresql database admin.","title":"DevOps"},{"location":"devops/bash/","text":"# fetch which processes listen or have a conn established on a port lsof -i | grep -E \"(LISTEN|ESTABLISHED)\" # check which process specifically listens/established a conn on port 8000. # You can use any port number as input lsof -i :8000 # kill process based on process ID kill -9 <PID>","title":"Bash"},{"location":"devops/setup-docs/getting-started/","text":"Getting started with Mkdocs \u00b6 This page explains how to use MkDocs and host it on Github Pages. Create mkdocs site \u00b6 pip3 install mkdocs installs the mkdocs package to generate python based static sites. mkdocs new [site-name] creates a new project with the correct set-up. mkdocs serve - Start the live-reloading docs server. mkdocs build subsequently generates your static pages and places the files in /sites . Make sure to include this in your .gitignore file. Setting up a Python environment \u00b6 Here we explain how to set up a virtual environment using pyenv and use requirements.txt to keep track of our dependencies. Instead of using pyenv you can also directly start with Poetry. Further down this guide we explain the steps regarding a Poetry set up. setup a python environment ( pyenv virtualenv [environmentName] ) and activate it with pyenv activate [environmentName] . Install packages and plugins accordingly. Run pip freeze > requirements.txt to output your deps into requirements.txt . Add .venv to your .gitignore file. Host on gh pages \u00b6 Create a new repository in Github (empty one). Perform git init in your local mkdocs folder (the root folder that stores the mkdocs.yml file). The usual stuff (git add, git commit, git remote add origin [remote-url] and git push -u origin main). Now you need to perform the mkdocs gh-deploy command in your local mkdocs project. This creates a Git branch names gh-pages for Github Pages to pick up your site. Running this command will generate the static website. Rather than outputting the files in the site folder as we saw in the previous post, the website will be saved in a new branch named gh-pages and a push of this branch is done towards GitHub. If you go to the Actions tab on the GitHub repository, you will see GitHub automatically picking up the new gh-pages branch and deploying it to GitHub pages. Now your site is hosted on github pages accordingly. Everything has been set up! Automated deployments \u00b6 So far I have described how to manually deploy your mkdocs site to Github Pages. The next steps explain how you can do this automatically through Github Actions. Go to your Github repo and then to Actions menu. Click on New workflow and select the suggested workflow (simple workflow). This will generate a new file called blank.yml in the .github/workflows directory. See the example below for the changes that we conducted on the template file. I suggest to copy this over. Please be mindful of your default branch's name! If you have master , make sure to reflect that in the yaml file below as well. --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt - name : Build site run : mkdocs gh-deploy --force --clean --verbose Change the file name to gh-pages.yml and make sure to commit this change to the default branch. Remember that the gh-pages branch will be generated by MkDocs, and we never want to either check out this branch locally or modify it manually. Now you have a minimal set-up for deploying your site through Github Actions automatically. The next steps refines our Github Workflow approach. These steps are not really needed if you don't want to. You can continue using pip3 with pyenv (or venv) for dependency and environment management, instead of poetry. You may also decide to add a Deploy job without integrating with Poetry all together. You simply change the yml file as follows: # I will not include the stuff that won't be changed - name : Build site run : mkdocs build --verbose # here I have updated the code from gh-deploy to build # When you replace gh-deploy, you need to add the step below in order to push the site to gh-pages: - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} # ----- SEE NEXT SECTION FOR DEPLOYMENT KEYS! ----- publish_dir : ./site Refining Github Workflow \u00b6 After our first deployment, we do a git fetch and git pull in our local project. This will update our local folder and adds the newly created contents inside the .github directory. Before we use Poetry locally, we first need to make sure that we even have a Poetry environment and pyproject. toml file in our root directory. We run poetry init , and install the dependencies that are listed in our requirements.txt file. Now we tweak the gh-pages.yml file as follows: --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install Poetry uses : snok/install-poetry@v1 with : virtualenvs-create : true virtualenvs-in-project : true installer-parallel : true - name : Load cached venv id : cached-poetry-dependencies uses : actions/cache@v3 with : path : .venv key : venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }} - name : Install dependencies if cache doesnt exist if : steps.cached-poetry-dependencies.outputs.cache-hit != 'true' run : poetry install --no-interaction --no-root - name : Install project run : poetry install --no-interaction --no-root - name : Make the site run : poetry run mkdocs build --verbose # run: poetry run mkdocs gh-deploy --force --clean --verbose - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./site As shown above, we add a couple of jobs that take care of installing Poetry and our project dependencies. We also replace our run command in the Make the site job: we add poetry but also replace gh-deploy with build . Again, you may choose to keep poetry run mkdocs gh-deploy . Doing so, renders the job Deploy redundant. I suggest to remove that job accordingly. Since we are newly introducing Poetry to our pipeline, it might be good to first test if this is integrated correctly, and we don't experience any issues. If everything is fine, we can work on the next step. Deploy job \u00b6 Now that we don't use the gh-deploy functionality provided by MkDocs , we need to add another job that takes care of pushing our site to the gh-pages branch. That's where the Deploy job comes in. You'll notice the following parameter: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} . Because we don't make use of the gh-deploy functionality, we need to make use of Github Runners : agents that are used to deploy code to certain environments. In our case, it is the gh-pages environment (i.e., branch). However, we need to properly authenticate the Runner otherwise it will not be able to perform write operations, thus deploying our site. To ensure that the Runner is able to authenticate, we make use of deploy keys and add them to our Github project. See the steps below for doing this: on your device, go to .ssh directory and run: ssh-keygen -t ed25519 -C \" $( git config user.email ) \" -f [ ProjectName ] -N \"\" The [name] is the name that is used to write the public and private key files. I suggest to use the name of your project. This command generates a private and public key using the ed25519 algorithm. Go to your Github project settings > deploy keys and click on add deploy key title = Public key of ACTIONS_DEPLOY_KEY paste the contents of your [ProjectName].pub file into this field. This is your public key that you just generated. Go to Github project settings > Secrets > Actions and click on new repository secret secret name = ACTIONS_DEPLOY_KEY > you set this as title, since you refer to this variable name in your gh-pages.yml file ( deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} ) paste the contents of your [ProjectName] file in the .ssh folder. This is the private key that you just generated. Now you are all set. Now you can push your changes to your Github default branch. If there are no unexpected issues, Github will automatically deploy your changes to the Github Pages site, based on your workflow. References \u00b6 See the links below for guides and other docs that helped me understand Mkdocs and deploying to Github Pages: Creating a documentation site with mkdocs Hosting a MkDocs-driven documentation site on GitHub Pages Deploying a Mkdocs site with Github Actions","title":"Getting started"},{"location":"devops/setup-docs/getting-started/#getting-started-with-mkdocs","text":"This page explains how to use MkDocs and host it on Github Pages.","title":"Getting started with Mkdocs"},{"location":"devops/setup-docs/getting-started/#create-mkdocs-site","text":"pip3 install mkdocs installs the mkdocs package to generate python based static sites. mkdocs new [site-name] creates a new project with the correct set-up. mkdocs serve - Start the live-reloading docs server. mkdocs build subsequently generates your static pages and places the files in /sites . Make sure to include this in your .gitignore file.","title":"Create mkdocs site"},{"location":"devops/setup-docs/getting-started/#setting-up-a-python-environment","text":"Here we explain how to set up a virtual environment using pyenv and use requirements.txt to keep track of our dependencies. Instead of using pyenv you can also directly start with Poetry. Further down this guide we explain the steps regarding a Poetry set up. setup a python environment ( pyenv virtualenv [environmentName] ) and activate it with pyenv activate [environmentName] . Install packages and plugins accordingly. Run pip freeze > requirements.txt to output your deps into requirements.txt . Add .venv to your .gitignore file.","title":"Setting up a Python environment"},{"location":"devops/setup-docs/getting-started/#host-on-gh-pages","text":"Create a new repository in Github (empty one). Perform git init in your local mkdocs folder (the root folder that stores the mkdocs.yml file). The usual stuff (git add, git commit, git remote add origin [remote-url] and git push -u origin main). Now you need to perform the mkdocs gh-deploy command in your local mkdocs project. This creates a Git branch names gh-pages for Github Pages to pick up your site. Running this command will generate the static website. Rather than outputting the files in the site folder as we saw in the previous post, the website will be saved in a new branch named gh-pages and a push of this branch is done towards GitHub. If you go to the Actions tab on the GitHub repository, you will see GitHub automatically picking up the new gh-pages branch and deploying it to GitHub pages. Now your site is hosted on github pages accordingly. Everything has been set up!","title":"Host on gh pages"},{"location":"devops/setup-docs/getting-started/#automated-deployments","text":"So far I have described how to manually deploy your mkdocs site to Github Pages. The next steps explain how you can do this automatically through Github Actions. Go to your Github repo and then to Actions menu. Click on New workflow and select the suggested workflow (simple workflow). This will generate a new file called blank.yml in the .github/workflows directory. See the example below for the changes that we conducted on the template file. I suggest to copy this over. Please be mindful of your default branch's name! If you have master , make sure to reflect that in the yaml file below as well. --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt - name : Build site run : mkdocs gh-deploy --force --clean --verbose Change the file name to gh-pages.yml and make sure to commit this change to the default branch. Remember that the gh-pages branch will be generated by MkDocs, and we never want to either check out this branch locally or modify it manually. Now you have a minimal set-up for deploying your site through Github Actions automatically. The next steps refines our Github Workflow approach. These steps are not really needed if you don't want to. You can continue using pip3 with pyenv (or venv) for dependency and environment management, instead of poetry. You may also decide to add a Deploy job without integrating with Poetry all together. You simply change the yml file as follows: # I will not include the stuff that won't be changed - name : Build site run : mkdocs build --verbose # here I have updated the code from gh-deploy to build # When you replace gh-deploy, you need to add the step below in order to push the site to gh-pages: - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} # ----- SEE NEXT SECTION FOR DEPLOYMENT KEYS! ----- publish_dir : ./site","title":"Automated deployments"},{"location":"devops/setup-docs/getting-started/#refining-github-workflow","text":"After our first deployment, we do a git fetch and git pull in our local project. This will update our local folder and adds the newly created contents inside the .github directory. Before we use Poetry locally, we first need to make sure that we even have a Poetry environment and pyproject. toml file in our root directory. We run poetry init , and install the dependencies that are listed in our requirements.txt file. Now we tweak the gh-pages.yml file as follows: --- name : CI on : push : branches : [ \"main\" ] pull_request : branches : [ \"main\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : jobs : build : runs-on : ubuntu-latest steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Set up python uses : actions/setup-python@v4 with : python-version : '3.10' - name : Install Poetry uses : snok/install-poetry@v1 with : virtualenvs-create : true virtualenvs-in-project : true installer-parallel : true - name : Load cached venv id : cached-poetry-dependencies uses : actions/cache@v3 with : path : .venv key : venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }} - name : Install dependencies if cache doesnt exist if : steps.cached-poetry-dependencies.outputs.cache-hit != 'true' run : poetry install --no-interaction --no-root - name : Install project run : poetry install --no-interaction --no-root - name : Make the site run : poetry run mkdocs build --verbose # run: poetry run mkdocs gh-deploy --force --clean --verbose - name : Deploy uses : peaceiris/actions-gh-pages@v3 if : ${{ github.ref == 'refs/heads/main' }} with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./site As shown above, we add a couple of jobs that take care of installing Poetry and our project dependencies. We also replace our run command in the Make the site job: we add poetry but also replace gh-deploy with build . Again, you may choose to keep poetry run mkdocs gh-deploy . Doing so, renders the job Deploy redundant. I suggest to remove that job accordingly. Since we are newly introducing Poetry to our pipeline, it might be good to first test if this is integrated correctly, and we don't experience any issues. If everything is fine, we can work on the next step.","title":"Refining Github Workflow"},{"location":"devops/setup-docs/getting-started/#deploy-job","text":"Now that we don't use the gh-deploy functionality provided by MkDocs , we need to add another job that takes care of pushing our site to the gh-pages branch. That's where the Deploy job comes in. You'll notice the following parameter: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} . Because we don't make use of the gh-deploy functionality, we need to make use of Github Runners : agents that are used to deploy code to certain environments. In our case, it is the gh-pages environment (i.e., branch). However, we need to properly authenticate the Runner otherwise it will not be able to perform write operations, thus deploying our site. To ensure that the Runner is able to authenticate, we make use of deploy keys and add them to our Github project. See the steps below for doing this: on your device, go to .ssh directory and run: ssh-keygen -t ed25519 -C \" $( git config user.email ) \" -f [ ProjectName ] -N \"\" The [name] is the name that is used to write the public and private key files. I suggest to use the name of your project. This command generates a private and public key using the ed25519 algorithm. Go to your Github project settings > deploy keys and click on add deploy key title = Public key of ACTIONS_DEPLOY_KEY paste the contents of your [ProjectName].pub file into this field. This is your public key that you just generated. Go to Github project settings > Secrets > Actions and click on new repository secret secret name = ACTIONS_DEPLOY_KEY > you set this as title, since you refer to this variable name in your gh-pages.yml file ( deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} ) paste the contents of your [ProjectName] file in the .ssh folder. This is the private key that you just generated. Now you are all set. Now you can push your changes to your Github default branch. If there are no unexpected issues, Github will automatically deploy your changes to the Github Pages site, based on your workflow.","title":"Deploy job"},{"location":"devops/setup-docs/getting-started/#references","text":"See the links below for guides and other docs that helped me understand Mkdocs and deploying to Github Pages: Creating a documentation site with mkdocs Hosting a MkDocs-driven documentation site on GitHub Pages Deploying a Mkdocs site with Github Actions","title":"References"},{"location":"devops/setup-docs/tools-plugins/","text":"Tools \u00b6 markdownlint-cli \u00b6 When you have a project with markdown ( .md ) files, or when building documentation sites based on markdown files (e. g., with Mkdocs), you can use markdownlint-cli to improve your markdown code. Install with homebrew: brew install markdownlint-cli It uses a default ruleset to check your files, but you can also tweak that ruleset by including a .markdownlint. yml or .markdownlint.jsonc file in your project's root. In this file you can change the ruleset. See this yml file for an example. This file also explains the rules and their behaviour. template \u00b6 I use the following .markdownlint.yml template: --- default : true # MD003/heading-style/header-style - Heading style MD003 : style : atx # MD004/ul-style - Unordered list style MD004 : style : asterisk # MD007/ul-indent - Unordered list indentation MD007 : indent : 4 # MD013/line-length - Line length MD013 : false # MD025/single-title/single-h1 - Multiple top-level headings # in the same document MD025 : false # MD030/list-marker-space - Spaces after list markers MD030 : ul_multi : 1 ol_multi : 2 # MD035/hr-style - Horizontal rule style MD035 : style : --- # MD046/code-block-style - Code block style MD046 : false pre-commit \u00b6 pre-commit helps you implement checks before committing code. You install it through pip or poetry: poetry add pre-commit . Subsequently, you create a .pre-commit-config.yml file where you include the checks that must be performed. Set up the pre-commit hooks for your project: pre-commit install Now, every time you commit code, the pre-commit hook will check your code based on the checks included in your config file. Manually trigger a pre-commit check: pre-commit run --all-files Plugins \u00b6 There are alot of plugins that you can use when building a doc site with Mkdocs and Material theme. Some of them are listed below. markdown_extensions \u00b6 admonitions \u00b6 Admonitions , also known as call-outs, are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several types of admonitions and allows for the inclusion and nesting of arbitrary content. To enable this extension, add the following to mkdocs.yml : markdown_extensions : - admonition - pymdownx.details - pymdownx.superfences Admonitions follow a simple syntax: a block starts with !!!, followed by a single keyword used as a type qualifier. The content of the block follows on the next line, indented by four spaces: Note This is a dummy block of type 'note'. Custom title This is a dummy block of type 'note' with a custom title. Info This is a collapsable dummy block of type 'info'. Bug This is a dummy block of type 'bug'. You can do all kinds of stuff with this extension. Check the url above for all the variation you can use.","title":"Tools & plugins"},{"location":"devops/setup-docs/tools-plugins/#tools","text":"","title":"Tools"},{"location":"devops/setup-docs/tools-plugins/#markdownlint-cli","text":"When you have a project with markdown ( .md ) files, or when building documentation sites based on markdown files (e. g., with Mkdocs), you can use markdownlint-cli to improve your markdown code. Install with homebrew: brew install markdownlint-cli It uses a default ruleset to check your files, but you can also tweak that ruleset by including a .markdownlint. yml or .markdownlint.jsonc file in your project's root. In this file you can change the ruleset. See this yml file for an example. This file also explains the rules and their behaviour.","title":"markdownlint-cli"},{"location":"devops/setup-docs/tools-plugins/#template","text":"I use the following .markdownlint.yml template: --- default : true # MD003/heading-style/header-style - Heading style MD003 : style : atx # MD004/ul-style - Unordered list style MD004 : style : asterisk # MD007/ul-indent - Unordered list indentation MD007 : indent : 4 # MD013/line-length - Line length MD013 : false # MD025/single-title/single-h1 - Multiple top-level headings # in the same document MD025 : false # MD030/list-marker-space - Spaces after list markers MD030 : ul_multi : 1 ol_multi : 2 # MD035/hr-style - Horizontal rule style MD035 : style : --- # MD046/code-block-style - Code block style MD046 : false","title":"template"},{"location":"devops/setup-docs/tools-plugins/#pre-commit","text":"pre-commit helps you implement checks before committing code. You install it through pip or poetry: poetry add pre-commit . Subsequently, you create a .pre-commit-config.yml file where you include the checks that must be performed. Set up the pre-commit hooks for your project: pre-commit install Now, every time you commit code, the pre-commit hook will check your code based on the checks included in your config file. Manually trigger a pre-commit check: pre-commit run --all-files","title":"pre-commit"},{"location":"devops/setup-docs/tools-plugins/#plugins","text":"There are alot of plugins that you can use when building a doc site with Mkdocs and Material theme. Some of them are listed below.","title":"Plugins"},{"location":"devops/setup-docs/tools-plugins/#markdown_extensions","text":"","title":"markdown_extensions"},{"location":"devops/setup-docs/tools-plugins/#admonitions","text":"Admonitions , also known as call-outs, are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several types of admonitions and allows for the inclusion and nesting of arbitrary content. To enable this extension, add the following to mkdocs.yml : markdown_extensions : - admonition - pymdownx.details - pymdownx.superfences Admonitions follow a simple syntax: a block starts with !!!, followed by a single keyword used as a type qualifier. The content of the block follows on the next line, indented by four spaces: Note This is a dummy block of type 'note'. Custom title This is a dummy block of type 'note' with a custom title. Info This is a collapsable dummy block of type 'info'. Bug This is a dummy block of type 'bug'. You can do all kinds of stuff with this extension. Check the url above for all the variation you can use.","title":"admonitions"},{"location":"health/fitness/","text":"This is page is meant to document my thoughts on fitness and how to optimise your work outs alongside a full time job and other activities that you want to pursue. Schemas \u00b6 I currently do full body workouts, split between two different schemas. Each schema has two major exercises: the low bar squat and the deadlift . Each workout also has pull-ups with the purpose of breaking through the plateau of 25 reps in total. Tips & Tricks \u00b6 Squats \u00b6 Squats seem straightforward, but are actually one of the most technical exercises that you can do. To maintain a good technique and optimise for gaining weights, I have created a list op tips and tricks below. These tricks assume a low bar position. Don't use a pad to soften the bar on your back! This impairs your technique and causes you to lift less weight (the pad absorbs part of the weight) Do shoulder rotation exercises before you start. The low bar position might make your shoulder uncomfortable. When your shoulder starts getting too uncomfortable you might naturally bend your wrist to compensate. To counteract that, use a thumbless grip (thumbs over the bar). This allows you to maintain a straight wrist position and lock the bar. Don't move your elbows back and forth. This moves the bar and fucks up your technique. To lock your elbows, act like you are bending the barbell towards ths sides of your chest. Move downwards in a smooth way. Don't go slow, use the momentum in your muscles at the bottom, to go up again. Think of 'bouncing' at the bottom. Use that momentum. Going too slow will impair your technique. When you are at the bottom, move your ass up first. Think of a cord that is attached to your ass. This will train the hamstring and glutes better! youtube vids on squat technique Check our the following videos by Alan Thrall on squat technique: Low bar squat - Rack position & Shoulder warm-ups How to squat - Low bar","title":"Fitness"},{"location":"health/fitness/#schemas","text":"I currently do full body workouts, split between two different schemas. Each schema has two major exercises: the low bar squat and the deadlift . Each workout also has pull-ups with the purpose of breaking through the plateau of 25 reps in total.","title":"Schemas"},{"location":"health/fitness/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"health/fitness/#squats","text":"Squats seem straightforward, but are actually one of the most technical exercises that you can do. To maintain a good technique and optimise for gaining weights, I have created a list op tips and tricks below. These tricks assume a low bar position. Don't use a pad to soften the bar on your back! This impairs your technique and causes you to lift less weight (the pad absorbs part of the weight) Do shoulder rotation exercises before you start. The low bar position might make your shoulder uncomfortable. When your shoulder starts getting too uncomfortable you might naturally bend your wrist to compensate. To counteract that, use a thumbless grip (thumbs over the bar). This allows you to maintain a straight wrist position and lock the bar. Don't move your elbows back and forth. This moves the bar and fucks up your technique. To lock your elbows, act like you are bending the barbell towards ths sides of your chest. Move downwards in a smooth way. Don't go slow, use the momentum in your muscles at the bottom, to go up again. Think of 'bouncing' at the bottom. Use that momentum. Going too slow will impair your technique. When you are at the bottom, move your ass up first. Think of a cord that is attached to your ass. This will train the hamstring and glutes better! youtube vids on squat technique Check our the following videos by Alan Thrall on squat technique: Low bar squat - Rack position & Shoulder warm-ups How to squat - Low bar","title":"Squats"},{"location":"health/sleep/","text":"Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli. References This section is mostly extracted from Lyz's article on Sleep , which in turn is based on the why we sleep book by Matthew Walker .","title":"Sleep"}]}